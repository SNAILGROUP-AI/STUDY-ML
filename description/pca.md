## π μ°¨μ›μ μ €μ£Ό

- **μ°¨μ›μ μ €μ£Ό(The curse of dimensionality)**
    - μ •μ : κ³ μ°¨μ› κ³µκ°„μ—μ„ λ°μ΄ν„°λ¥Ό λ¶„μ„ν•κ³  μ •λ¦¬ν•  λ• λ°μƒν•λ” λ‹¤μ–‘ν• ν„μƒ
    - ν„μƒ : μ°¨μ›μ΄ μ¦κ°€ν•λ©΄ κ³µκ°„μ λ¶€ν”Όκ°€ λ„λ¬΄ λΉ¨λ¦¬ μ¦κ°€ν•μ—¬ μ‚¬μ© κ°€λ¥ν• λ°μ΄ν„°κ°€ μƒλ€μ μΌλ΅ ν¬μ†ν•΄μ§
    - λ¬Έμ μ  : κ°λ³„ μ°¨μ› λ‚΄ ν•™μµν•  λ°μ΄ν„° κ°μκ°€ ν¬μ†ν•΄μ Έμ„ ν•™μµμ΄ μ λ€λ΅ μ΄λ£¨μ–΄μ§€μ§€ μ•μ
    - ν•΄λ²• : μ°¨μ› μ¶•μ†

- **μ°¨μ› μ¶•μ†(dimensionality reduction)**
    - **μ •μ : λ°μ΄ν„° μ„ΈνΈμ μ°¨μ›, κ³§ μ„¤λ…λ³€μμ κ°μλ¥Ό μ¤„μ΄λ” μ‘μ—…**

    - **μ§€λ„ν•™μµμ—μ„μ μ°¨μ› μ¶•μ† : μ„¤λ…λ³€μμ™€μ μƒκ΄€κ΄€κ³„λ¥Ό κ°€μ •ν• μƒνƒμ—μ„ μ§„ν–‰λ¨**
        - λ¶„λ¥λ¶„μ„ : μΉμ‚°λΉ„ κΈ°μ¤€ μ μλ―Έν•μ§€ μ•μ€ μ„¤λ…λ³€μ μ κ±°
        - μ„ ν•νκ·€λ¶„μ„ : μ„ ν•νκ·€λ¶„μ„μ 4κ° κ°€μ • ν•μ— λ¶„μ‚°ν½μ°½κ³„μ κΈ°μ¤€ λ‹¤μ¤‘κ³µμ„ μ„±μ΄ λ‚®μ§€ μ•μ€ μ„¤λ…λ³€μ μ κ±°
    
    - **λΉ„μ§€λ„ν•™μµμ—μ„μ μ°¨μ› μ¶•μ†**
        - μ„¤λ…λ³€μμ— κ΄€ν• μ •λ³΄κ°€ μ—†μΌλ―€λ΅ μ§€λ„ν•™μµ μ‹ μ°¨μ› μ¶•μ† κΈ°λ²•λ“¤μ„ μ‚¬μ©ν•  μ μ—†μ
        - λ”°λΌμ„ λ‹¤μμ μ°¨μ›λ“¤μ„ λ€ν‘ν•  μ μλ” μ£Όμ”ν• μ„±λ¶„λ“¤μ„ μ„ λ³„ν•¨μΌλ΅μ¨ μ°¨μ›μ„ μ¶•μ†ν•΄μ•Ό ν•¨
        - ν¬μ(Projection), λ§¤λ‹ν΄λ“ ν•™μµ(Manifold Learning), μ£Όμ„±λ¶„ λ¶„μ„(PCA) λ“±

---

## π§β€β™€οΈ μ£Όμ„±λ¶„ λ¶„μ„

<details><summary><h3>μ£Όμ„±λ¶„ λ¶„μ„(Principle Component Analysis; PCA)</h3></summary>

- **μ£Όμ„±λ¶„ λ¶„μ„(Principle Component Analysis)**

    - **μ •μ : λ³€μ κ°„μ— μ΅΄μ¬ν•λ” μƒκ΄€κ΄€κ³„λ¥Ό μ΄μ©ν•μ—¬ μ΄λ¥Ό λ€ν‘ν•λ” μ£Όμ„±λ¶„μ„ μ¶”μ¶ν•μ—¬ μ°¨μ›μ„ μ¶•μ†ν•λ” κΈ°λ²•**

    - **μ΄μ : μ •λ³΄(νΉμ„±) μ μ‹¤ λ¬Έμ **

        ![04AD38E4-3544-4BEC-952C-0B4542AA1538](https://user-images.githubusercontent.com/116495744/224222113-e15b8091-9a64-4a49-bd7d-916d4bb75874.jpg)

    - **ν•΄λ²• : λ¶„μ‚°μ„ μµλ€ν• λ³΄μ΅΄ν•¨μΌλ΅μ¨ λ μ½”λ“ κ°„ νΉμ„±λ³„ μ°¨μ΄λ¥Ό λ³΄μ΅΄ν•¨**

        ![IMG_7017](https://user-images.githubusercontent.com/116495744/224222115-02d0ecb3-112d-4417-a39f-8d69f91ad84f.jpg)

</details>

<details><summary><h3>μ§κ΄€μ  μ΄ν•΄</h3></summary>

- **Whitening**

    ![IMG_7004](https://user-images.githubusercontent.com/116495744/224222107-98d84b92-79bd-47c0-b430-aa2584b9e22f.JPG)

    - Nκ°μ μ„¤λ…λ³€μμ— λ€ν•μ—¬ λ¨λ“  μ„¤λ…λ³€μμ ν‰κ· μ„ μ›μ μΌλ΅ ν•λ” Nμ°¨μ› κ·Έλν”„λ¥Ό μƒμ„±ν•¨
    - λ°μ΄ν„° μ„ΈνΈλ¥Ό κ·Έλν”„μ— λ¬μ‚¬ν•¨

- **μ£Όμ„±λ¶„ μ¶”μ¶**

    ![μ‚¬μ](https://user-images.githubusercontent.com/116495744/224226095-898ac9a8-9cec-4b0d-a553-074bbc6a1ffd.jpeg)

    - μ›μ μ„ μ§€λ‚λ” μ§μ„  μ¤‘μ—μ„ λ¨λ“  λ μ½”λ“λ¥Ό μ‚¬μν–μ„ λ• SSκ°€ κ°€μ¥ ν° μ§μ„ μ„ μ°Ύμ
    - μ›μ μ„ μ§€λ‚κ³  μ•μ„ κµ¬ν• μ§μ„ κ³Ό μ§κµν•λ©΄μ„ SSκ°€ κ°€μ¥ ν° μ§μ„ μ„ μ°Ύμ
    - μ›μ μ„ μ§€λ‚κ³  μ•μ„ κµ¬ν• μ§μ„ λ“¤κ³Ό μ§κµν•λ©΄μ„ SSκ°€ κ°€μ¥ ν° μ§μ„ μ„ μ°Ύμ
    - μ„ κ³Όμ •μ„ λ°λ³µν•λ©΄μ„ μ°¨μ›μ κ°―μλ§νΌμ μ§μ„ μ„ μ°Ύμ

- **μ£Όμ„±λ¶„ μ„ λ³„**
    - **μ£Όμ„±λ¶„(Principle Component; PC)**
        - μ„ μ μ°¨λ¥Ό ν†µν•΄ μ°Ύμ€ μ§μ„ λ“¤μ„ ν•΄λ‹Ή λ°μ΄ν„° μ„ΈνΈμ μ£Όμ„±λ¶„μ΄λΌκ³  μ •μν•¨
    
    - **SS(Sum of Squared Distance)**
        - μ›μ κ³Ό μ‚¬μμ  κ°„ κ±°λ¦¬ μ κ³±μ ν•©μ„ ν•΄λ‹Ή μ£Όμ„±λ¶„μ SSλΌκ³  μ •μν•¨
        - μ›μ κ³Ό νΉμ • λ μ½”λ“μ μ‚¬μμ  κ°„ κ±°λ¦¬μ μ κ³±μ„ ν•΄λ‹Ή λ μ½”λ“μ μ£Όμ„±λ¶„κ°’μΌλ΅ ν•΄μ„ν•¨
        - μ „μ²΄ μ£Όμ„±λ¶„μ SS λ€λΉ„ νΉμ • μ£Όμ„±λ¶„μ SSλ¥Ό ν•΄λ‹Ή μ§μ„ μ΄ μ „μ²΄ νΉμ„±μ„ μ„¤λ…ν•λ” μ •λ„λ΅ ν•΄μ„ν•¨
    
    - **μ£Όμ„±λ¶„ μ„ λ³„**
        - Nμ°¨μ› λ°μ΄ν„° μ„ΈνΈλ¥Ό kμ°¨μ›μΌλ΅ μ¤„μ΄κ³ μ ν•λ” κ²½μ°
        - SS κΈ°μ¤€ μƒμ„ kκ° μ£Όμ„±λ¶„μ„ μ¶”μ¶ν•¨

</details>

<details><summary><h3>μν•™μ  μ΄ν•΄</h3></summary>

- **μ£Όμ” κ°λ…**
    - **λ¶„μ‚°(Variance; Var)**

        $$var(X) = \displaystyle\sum_{i=0}^{n}\frac{(X-\overline{X})^2}{n}$$

        - μ •μ : λ‹¨μ°¨μ› λ°μ΄ν„° μ„ΈνΈμ— λ€ν•μ—¬ ν‰κ· μ μ„ μ¤‘μ‹¬μΌλ΅ λ μ½”λ“κ°€ ν©μ–΄μ§„ μ •λ„

    - **κ³µλ¶„μ‚°(Covariance; Cov)**

        $$cov(X, Y) = \displaystyle\sum_{i=0}^{n}\frac{(X_i-\overline{X})(Y_i-\overline{Y})}{n}$$

        - μ •μ : λ‹¤μ°¨μ› λ°μ΄ν„° μ„ΈνΈμ— λ€ν•μ—¬ ν‰κ· μ μ„ μ¤‘μ‹¬μΌλ΅ λ μ½”λ“κ°€ ν©μ–΄μ§„ μ •λ„
        - ν•΄μ„ : 2κ°μ μ¶•μ„ κ°€μ •ν–μ„ λ•, ν• λ³€μμ μ¦κ°μ— λ”°λ¥Έ λ‹¤λ¥Έ λ³€μμ μ¦κ° κ²½ν–¥μ„±

    - **κ³µλ¶„μ‚°ν–‰λ ¬(Covariance Matrix)**

        $$ \sum = 
        \begin{pmatrix}
        var(X) & cov(X, Y) \\
        cov(Y, X) & var(Y)
        \end{pmatrix} $$

        - **μ •μ**
            - λ‹¤μ°¨μ› λ°μ΄ν„° μ„ΈνΈλ¥Ό κµ¬μ„±ν•λ” λ³€μ(νΉμ€ μ¶•) $X, Y, Z, \cdots$ μ— λ€ν•μ—¬
            - $i$ λ²μ§Έ, $j$ λ²μ§Έ λ³€μ(νΉμ€ μ¶•)μ κ³µλ¶„μ‚°μ„ $(i, j)$ μ κ°’μΌλ΅ κ°€μ§€λ” μ •λ°©ν–‰λ ¬

        - **μƒκ΄€κ΄€κ³„μ™€ κ³µλ¶„μ‚°ν–‰λ ¬**
            - **μƒκ΄€ν–‰λ ¬(Correlation Matrix)** : κ³µλ¶„μ‚°ν–‰λ ¬μ„ μ •κ·ν™”ν• ν–‰λ ¬
            - **ν”Όμ–΄μ¨ μƒκ΄€κ³„μ(Pearson Correlation Coefficient)** : μƒκ΄€ν–‰λ ¬μ„ κµ¬μ„±ν•λ” μ¤μΉΌλΌ

        - **μ„ ν•λ³€ν™κ³Ό κ³µλ¶„μ‚°ν–‰λ ¬**

            ![κ³µλ¶„μ‚°ν–‰λ ¬κ³Ό κ³ μ λ²΅ν„°](https://user-images.githubusercontent.com/116495744/224226188-05975c29-4ac8-4572-b796-fb7eec3bab5a.jpeg)

            - μ„μμ ν–‰λ ¬ Pμ— λ€ν•μ—¬ κ·Έ κ³µλ¶„μ‚°ν–‰λ ¬μ„ ν–‰λ ¬ Qμ— λ‚΄μ ν•λ” κ²½μ°
            - κ·Έλν”„μƒμΌλ΅ ν‘ν„λ Qμ λ¶„ν¬κ°€ Pμ λ¶„ν¬μ™€ μ μ‚¬ν• ν•νƒλ΅ λ³€ν™λ¨

    - **κ³ μ λ²΅ν„°(EigenVector)μ™€ κ³ μ κ°’(EigenValue)**

        $$\sum \cdot V = \lambda \times V$$

        - **κ³ μ λ²΅ν„°(EigenVector)** : μ„μμ λ°μ΄ν„° μ„ΈνΈμ— λ€ν•μ—¬ κ·Έ κ³µλ¶„μ‚°ν–‰λ ¬μ„ λ‚΄μ ν•μ—¬ μ„ ν•λ³€ν™ν•λ”λΌλ„ λ°©ν–¥μ΄ λ³€ν™ μ „κ³Ό λ™μΌν• λ²΅ν„°
        - **κ³ μ κ°’(EigenValue)** : μ„μμ λ°μ΄ν„° μ„ΈνΈμ— λ€ν•μ—¬ κ·Έ κ³µλ¶„μ‚°ν–‰λ ¬μ„ λ‚΄μ ν•κΈ° μ „ κ³ μ λ²΅ν„°μ κΈΈμ΄ λ€λΉ„ λ‚΄μ ν• ν›„ κ³ μ λ²΅ν„°μ κΈΈμ΄
    
- **μ£Όμ„±λ¶„ λ¶„μ„μ μν•™μ  μ΄ν•΄**
    
    - **μ£Όμ„±λ¶„ μ¶”μ¶**
        - λ°μ΄ν„° μ„ΈνΈμ κ³µλ¶„μ‚°ν–‰λ ¬μ„ κµ¬ν•¨
        
        - κ³µλ¶„μ‚°ν–‰λ ¬μ κ³ μ λ²΅ν„°μ™€ κ³ μ κ°’μ„ κµ¬ν•¨
            - **κ³ μ λ²΅ν„°** : ν•΄λ‹Ή λ°μ΄ν„° μ„ΈνΈμ μ£Όμ„±λ¶„
            - **κ³ μ κ°’** : ν•΄λ‹Ή μ£Όμ„±λ¶„μ SS(Sum of Squared Distance)
    
    - **μ£Όμ„±λ¶„ μ„ λ³„**
        - κ³ μ λ²΅ν„°λ¥Ό κ³ μ κ°’ κΈ°μ¤€μΌλ΅ λ‚΄λ¦Όμ°¨μ μ •λ ¬
        - μ›ν•λ” μ°¨μ› μλ§νΌ κ³ μ λ²΅ν„°λ¥Ό μ„ λ³„

</details>

---

## π—’οΈ SK-Learnμ μ£Όμ„±λ¶„ λ¶„μ„ μ•κ³ λ¦¬μ¦

<details><summary><h3>μ‚¬μ© λ°©λ²•</h3></summary>

- **μ‚¬μ© λ°©λ²•**

    ```
    from sklearn.decomposition import PCA

    # PCA μ•κ³ λ¦¬μ¦ μΈμ¤ν„΄μ¤ μƒμ„±
    # μ¶•μ†ν•  μ°¨μ›μ μλ¥Ό 3μΌλ΅ μ„¤μ •
    pca = PCA(n_components = 3)

    # μ£Όμ„±λ¶„ νƒμƒ‰
    pca.fit(X)

    # λ°μ΄ν„° μ„ΈνΈ μ°¨μ› μ¶•μ†
    X = pca.transform(X)
    ```

- **μ£Όμ” ν•μ΄νΌνλΌλ―Έν„°**
    - `random_state = None`
    - `n_components` : μ¶•μ†ν•  μ°¨μ›μ κ°μ
    - `whiten = False` : μ›μ μ„ λ¨λ“  μ„¤λ…λ³€μλ“¤μ ν‰κ· μΌλ΅ μ΅°μ •ν•  κ²ƒμΈμ§€ μ—¬λ¶€

- **λ‹¤μμ μ†μ„±μ„ ν†µν•΄ ν›λ ¨λ λ¨λΈμ μ •λ³΄λ¥Ό ν™•μΈν•  μ μμ**
    - `n_samples_` : λ μ½”λ“ κ°μ
    - `n_features_` : μ¶•μ† μ „ μ°¨μ›μ κ°μ
    - `feature_names_in_` : μ¶•μ† μ „ μ°¨μ›λ…
    - `mean_` : μ¶•μ† μ „ μ°¨μ›λ³„ ν‰κ· 
    - `n_components_` : μ¶•μ† ν›„ μ°¨μ›μ κ°μ
    - `components_` : κ³ μ λ²΅ν„°
    - `explained_variance_` : κ° κ³ μ λ²΅ν„°μ κ³ μ κ°’
    - `explained_variance_ratio_` : μ „μ²΄ κ³ μ λ²΅ν„°μ κ³ μ κ°’ λ€λΉ„ κ° κ³ μ λ²΅ν„°μ κ³ μ κ°’

</details>

---

## π“ Practice

- [**μ‹¤μµ μ½”λ“**]()