## 💯 성능 평가란 무엇인가

- **정의 : 오차를 기준으로 모델의 성능을 평가하는 절차**
    - 오차 : 예측값과 실제값의 차이
    - 오차를 0으로 만드는 것은 현실적으로 불가능하므로 오차를 허용할 범위를 결정해야 함
    - 실제값이 주어져야 오차를 측정할 수 있으므로 지도학습에서만 이루어짐

- **목적 : 과적합 방지 및 최적 모델 채택**
    - **과적합(overfitting) 판단 기준**
        - 학습용 데이터를 통해 수행한 예측 오차와 평가용 데이터를 통해 수행한 예측 오차 간 차이가 적을수록 과적합되지 않았다고 판단함
        - 학습용 데이터로는 성능이 높게 평가되었으나 평가용 데이터로는 성능이 낮게 평가되었다면 학습용 데이터에 과대적합된 상태임
        - 반대로 학습용 데이터로는 성능이 낮게 평가되었으나 평가용 데이터로는 성능이 높게 평가되었다면 학습용 데이터에 과소적합된 상태임

    - **최적 모델 판단 기준**
        - 과적합 문제가 해결된 여러 모델들 중에서 성능이 가장 높은 모델을 채택하는 기준으로 사용 가능함
        - 즉, 최적 알고리즘 및 최적 하이퍼파라미터 판단 기준으로서 사용 가능함

- **주요 지표**
    - 모델링 목적 또는 반응변수의 유형에 따라 다른 평가지표를 사용함
    
        | 모델링 목적 | 반응변수 유형 | 주요 평가지표 |
        |---|---|---|
        | 분류분석 | 범주형 변수 | 정확도, F1-Score 등 |
        | 회귀분석 | 연속형 변수 | R2-Score, MSE, RMSE, MAE 등 |

---

## 📊 분류분석의 성능 평가 지표

<details><summary><h3>오차행렬</h3></summary>

- **오차행렬(confusion matrix)**
    - 분류분석 결과 예측 범주와 실제 범주를 교차 표(cross table) 형태로 정리한 행렬
    - 이항분류분석에 대하여 예측 오류가 얼마인지와 더불어 어떠한 유형의 예측 오류가 발생할 수 있는지 나타냄

![오차행렬](https://miro.medium.com/max/1400/1*4c3YSE9UrrmulLu0K66g1Q.png)

- **해석**
    - **True Positive(TP)** : 예측값이 1이고, 실제값도 1인 경우
    - **True Negative(TN)** : 예측값이 0이고, 실제값도 0인 경우
    - **False Positive(FP)** : 제1종 오류; 예측값이 1이고, 실제값은 0인 경우
    - **False Nagative(FN)** : 제2종 오류; 예측값이 0이고, 실제값은 1인 경우

- **정확도, 오류율, 정밀도, 재현율**

    - **정확도**

    ### $TP+TN \over TP+TN+FP+FN$
    
    - **오류율**

    ### $FP+FN \over TP+TN+FP+FN$
    
    - **정밀도**

    ### $TP \over TP+FP$
    
    - **재현율**

    ### $TP \over TP+FN$

![정확도, 오류율, 정밀도, 재현율의 관계](https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg)

</details>

<details><summary><h3>정확도(Accuracy)</h3></summary>

- **정의 : 실제 데이터와 예측 데이터가 얼마나 동일한지를 평가 기준으로 하는 지표**
    - 전체 예측 개수 대비 정확하게 예측한 개수
    - 0~1 사이의 값을 가지며 1에 가까울수록 성능이 우수하다고 평가함

- **주의 : 반응변수의 범주 간 개수가 불균형한 데이터 셋의 경우 활용하기에 적합하지 않음**
    - 가령 이항분류분석에서 참인 것의 개수가 99이고 거짓인 것의 개수가 1이라고 하자
    - 무조건 참으로 예측하면 0.99의 정확도를 가지게 됨

</details>

<details><summary><h3>F1-Score</h3></summary>

- **정의 : 정밀도와 재현율의 조화 평균**
    - 정밀도와 재현율 중 어느 한쪽으로 치우치지 않을수록 높은 값을 가짐
    - 0~1 사이의 값을 가지며 1에 가까울수록 성능이 우수하다고 평가함

- **정밀도**
    - 참으로 예측한 것의 개수 대비 정확하게 예측한 개수
    - 0~1 사이의 값을 가지며 1에 가까울수록 성능이 우수하다고 평가함
    - 제1종 오류가 문제되는 경우 주요한 지표로서 사용됨
    - 즉, 실제 거짓인 데이터를 참으로 판단하면 큰 문제가 발생하는 경우

- **재현율**
    - 참인 것의 개수 대비 참으로 예측한 것의 개수
    - 0~1 사이의 값을 가지며 1에 가까울수록 성능이 우수하다고 평가함
    - 민감도(sensitivity)라고도 부름
    - 제2종 오류가 문제되는 경우 주요한 지표로서 사용됨
    - 즉, 실제 참일 데이터를 거짓으로 판단하면 큰 문제가 되는 경우
    - 모델 안정성을 평가하는 지표로서 사용됨

- **정밀도와 재현율의 관계**
    - **정밀도와 재현율은 모두 TP를 높이는 것을 목적으로 함**
        - 단, 정밀도는 제1종 오류에 초점을 맞추는 지표로서 FP를 낮추는 방향으로 TP를 높이고자 함
        - 반면, 재현율은 제2종 오류에 초점을 맞추는 지표로서 FN를 낮추는 방향으로 TP를 높이고자 함

    - **정밀도와 재현율은 Trade-off 관계라고 볼 수 있음**
        - FP와 FN 중 어느 한쪽의 수치를 강제로 높이면 다른 한쪽의 수치를 낮추기 쉬워짐
        - 가령 어떤 자료가 참일 확률이 0.9라면 참으로 에측할 가능성이 매우 높음
        - 반면, 어떤 자료가 참일 확률이 0.1이라면 거짓으로 예측할 가능성이 매우 높음

    - **정밀도와 재현율의 조화평균이 가지는 의미**
        - 그렇다면 어떤 자료가 참일 확률이 0.6이라면 참과 거짓 중 무엇으로 분류해야 하는가
        - 임계값(threshold)을 기준으로 분류할 수 있음
        - F1-Score은 정확도와 재현율 중 어느 한쪽을 희생하지 않고서 양쪽을 모두 높이는 임계치임

</details>

<details><summary><h3>AUC</h3></summary>

- **ROC 곡선**
    - **민감도(True Positive Rate; TPR)** :  참인 것에 대하여 참으로 예측한 비율
    - **특이도(True Negative Rate; TNR)** : 거짓인 것에 대하여 거짓으로 예측한 비율
    - **ROC 곡선** : $1-TNR$ 의 변화에 따른 TPR의 변화 양상을 나타내는 곡선
    - $(x, y)=(0, 1)$ 일 때 성능이 가장 좋음

- **민감도와 특이도는 trade-off 관계임**
    - 모든 자료를 참으로 예측하는 경우 민감도를 최대치로 가져갈 수 있음
    - 반면, 이러한 경우 특이도를 최소치로 가져가게 됨

- **AUC(Area Under Curve)**
    - 정의 : ROC 곡선과 X축으로 둘러싸인 면적의 너비
    - 해석 : 0.5~1의 값을 가지며 1에 가까울수록 성능이 우수하다고 평가함
    - 척도 불변(Scale-Invariant) : 절대값이 아니라 비율을 통해서 성능을 평가함
    - 분류 임계값 불변(Classification-Threshold-Invariant) : 어떤 분류 임계값으로 무엇을 선택했는지와 무관하게 성능을 평가함

</details>

---

## 📈 회귀분석의 성능 평가 지표

<details><summary><h3>결정계수</h3></summary>

- 결정계수(coefficient of determination; r2-score)
    - 실제 값의 분산 대비 예측 값의 분산 비율
    - 0~1 사이의 값을 가지며, 값이 클수록 회귀식의 적합도가 높다고 판단함

</details>

<details><summary><h3>MSE & RMSE & MAE</h3></summary>

- 평균제곱오차(Mean Squared Error; MSE)
    - 오차(실제 값과 예측 값의 차이)를 제곱한 값의 평균
    - 값이 작을수록 회귀식의 적합도가 높다고 판단함
    - 오차를 제곱하므로 값을 과장할 수 있음

- 평균제곱근오차(Root Mean Squared Error; RMSE)
    - 평균제곱오차의 제곱근
    - 평균제곱오차에 제곱근하는 절차를 더하여 오차의 크기가 과장된 정도를 줄임

- 평균절대오차(Mean Absolute Error; MAE)
    - 오차(실제 값과 예측 값의 차이)의 절대값의 평균
    - 평균제곱오차에서 오차를 제곱하는 이유는 값의 방향성(음/양)이 아니라, 크기가 중요하기 때문임
    - 따라서 오차를 제곱한 값 대신 오차의 절대값을 활용하여 오차의 크기가 과장될 여지를 없앰

</details>