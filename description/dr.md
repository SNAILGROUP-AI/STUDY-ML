## π μ°¨μ›μ μ €μ£Ό

- **μ°¨μ›μ μ €μ£Ό(The curse of dimensionality)**
    - μ •μ : κ³ μ°¨μ› κ³µκ°„μ—μ„ λ°μ΄ν„°λ¥Ό λ¶„μ„ν•κ³  μ •λ¦¬ν•  λ• λ°μƒν•λ” λ‹¤μ–‘ν• ν„μƒ
    - ν„μƒ : μ°¨μ›μ΄ μ¦κ°€ν•λ©΄ κ³µκ°„μ λ¶€ν”Όκ°€ λ„λ¬΄ λΉ¨λ¦¬ μ¦κ°€ν•μ—¬ μ‚¬μ© κ°€λ¥ν• λ°μ΄ν„°κ°€ μƒλ€μ μΌλ΅ ν¬μ†ν•΄μ§
    - λ¬Έμ μ  : κ°λ³„ μ°¨μ› λ‚΄ ν•™μµν•  λ°μ΄ν„° κ°μκ°€ ν¬μ†ν•΄μ Έμ„ ν•™μµμ΄ μ λ€λ΅ μ΄λ£¨μ–΄μ§€μ§€ μ•μ
    - ν•΄λ²• : μ°¨μ› μ¶•μ†

- **μ°¨μ› μ¶•μ†(Dimensionality Reduction)**
    - **μ •μ : λ°μ΄ν„° μ„ΈνΈμ μ°¨μ›, κ³§ μ„¤λ…λ³€μμ κ°μλ¥Ό μ¤„μ΄λ” μ‘μ—…**

    - **μ°¨μ› μ¶•μ† λ°©λ²•**
        - **μ°¨μ› μ„ λ³„**
            - μ°¨μ› μ„ λ³„(Feature Selection)

        - **μ°¨μ› μ¶”μ¶**
            - μ„ ν• μ°¨μ› μ¶•μ†(Matrix Factor or Projection)
            - λΉ„μ„ ν• μ°¨μ› μ¶•μ†(Neighbor Graphs or Manifold Learning)

    - **μ§€λ„ν•™μµ : μ°¨μ› μ„ λ³„ λ°©μ‹**
        - λ°μ‘λ³€μμ— κ΄€ν• μ •λ³΄λ¥Ό λ°”νƒ•μΌλ΅ μ μλ―Έν•μ§€ μ•μ€ μ„¤λ…λ³€μλ¥Ό μ„ λ³„ν•μ—¬ μ κ±°ν•¨
        - λ¶„λ¥λ¶„μ„ : μΉμ‚°λΉ„ κΈ°μ¤€ μ μλ―Έν•μ§€ μ•μ€ μ„¤λ…λ³€μ μ κ±°
        - μ„ ν•νκ·€λ¶„μ„ : μ„ ν•νκ·€λ¶„μ„μ 4κ° κ°€μ • ν•μ— λ¶„μ‚°ν½μ°½κ³„μ κΈ°μ¤€ λ‹¤μ¤‘κ³µμ„ μ„±μ΄ λ‚®μ§€ μ•μ€ μ„¤λ…λ³€μ μ κ±°
    
    - **λΉ„μ§€λ„ν•™μµ : μ°¨μ› μ¶”μ¶ λ°©μ‹**
        - λ°μ‘λ³€μμ— κ΄€ν• μ •λ³΄κ°€ μ—†μΌλ―€λ΅ μ§€λ„ν•™μµμ μ°¨μ› μ„ λ³„ κΈ°λ²•λ“¤μ„ μ‚¬μ©ν•  μ μ—†μ
        - λ”°λΌμ„ λ‹¤μμ μ°¨μ›λ“¤μ„ λ€ν‘ν•  μ μλ” μ£Όμ”ν• μ„±λ¶„μ„ μ¶”μ¶ν•¨μΌλ΅μ¨ μ°¨μ›μ„ μ¶•μ†ν•΄μ•Ό ν•¨

---

## π§β€β™€οΈ μ£Όμ„±λ¶„ λ¶„μ„(Principle Component Analysis; PCA)

<details><summary><h3>μ£Όμ„±λ¶„ λ¶„μ„μ΄λ€ λ¬΄μ—‡μΈκ°€</h3></summary>

- **μ‚¬μ(Projection)**

    ![μ€ν•](https://t1.daumcdn.net/cfile/tistory/99CB343359F2DA5E07)

    - **μ •μ : μ„ ν• μ°¨μ› μ¶•μ† κΈ°λ²•**
        - κ³ μ°¨μ› μ…μ²΄μ ν•νƒλ¥Ό κ°€μ¥ μ λ‚νƒ€λ‚Ό μ μλ” μ €μ°¨μ› λ‹¨λ©΄μ„ μ°Ύλ” ν–‰μ„
    
    - **μΆ…λ¥**
        - μ£Όμ„±λ¶„ λ¶„μ„(Principle Component Analysis; PCA)
        - μ„ ν• νλ³„ λ¶„μ„(Linear Discriminant Analysis; LDA)
        - LDA(Linear Discriminant Analysis)
        - NMF(Non-Negative Matrix Factorization)

- **μ£Όμ„±λ¶„ λ¶„μ„(Principle Component Analysis)**

    - **μ •μ**
        - λ³€μ κ°„μ— μ΅΄μ¬ν•λ” μƒκ΄€κ΄€κ³„λ¥Ό μ΄μ©ν•μ—¬ μ΄λ¥Ό λ€ν‘ν•λ” μ£Όμ„±λ¶„μ„ μ¶”μ¶ν•μ—¬ μ°¨μ›μ„ μ¶•μ†ν•λ” κΈ°λ²•

    - **μ΄μ**

        ![04AD38E4-3544-4BEC-952C-0B4542AA1538](https://user-images.githubusercontent.com/116495744/224222113-e15b8091-9a64-4a49-bd7d-916d4bb75874.jpg)

        - μ •λ³΄(νΉμ„±) μ μ‹¤ λ¬Έμ 

    - **ν•΄λ²•**

        ![IMG_7017](https://user-images.githubusercontent.com/116495744/224222115-02d0ecb3-112d-4417-a39f-8d69f91ad84f.jpg)

        - λ¶„μ‚°μ„ μµλ€ν• λ³΄μ΅΄ν•¨μΌλ΅μ¨ λ μ½”λ“ κ°„ νΉμ„±λ³„ μ°¨μ΄λ¥Ό λ³΄μ΅΄ν•¨

</details>

<details><summary><h3>μ£Όμ„±λ¶„ λ¶„μ„μ μ΄ν•΄</h3></summary>

- **μ£Όμ„±λ¶„ λ¶„μ„μ μ§κ΄€μ  μ΄ν•΄**

    - **Whitening**

        ![IMG_7004](https://user-images.githubusercontent.com/116495744/224222107-98d84b92-79bd-47c0-b430-aa2584b9e22f.JPG)

        - Nκ°μ μ„¤λ…λ³€μμ— λ€ν•μ—¬ λ¨λ“  μ„¤λ…λ³€μμ ν‰κ· μ„ μ›μ μΌλ΅ ν•λ” Nμ°¨μ› κ·Έλν”„λ¥Ό μƒμ„±ν•¨
        - λ°μ΄ν„° μ„ΈνΈλ¥Ό κ·Έλν”„μ— λ¬μ‚¬ν•¨

    - **μ£Όμ„±λ¶„ μ¶”μ¶**

        ![μ‚¬μ](https://user-images.githubusercontent.com/116495744/224226095-898ac9a8-9cec-4b0d-a553-074bbc6a1ffd.jpeg)

        - μ›μ μ„ μ§€λ‚λ” μ§μ„  μ¤‘μ—μ„ λ¨λ“  λ μ½”λ“λ¥Ό μ‚¬μν–μ„ λ• SSκ°€ κ°€μ¥ ν° μ§μ„ μ„ μ°Ύμ
        - μ›μ μ„ μ§€λ‚κ³  μ•μ„ κµ¬ν• μ§μ„ κ³Ό μ§κµν•λ©΄μ„ SSκ°€ κ°€μ¥ ν° μ§μ„ μ„ μ°Ύμ
        - μ›μ μ„ μ§€λ‚κ³  μ•μ„ κµ¬ν• μ§μ„ λ“¤κ³Ό μ§κµν•λ©΄μ„ SSκ°€ κ°€μ¥ ν° μ§μ„ μ„ μ°Ύμ
        - μ„ κ³Όμ •μ„ λ°λ³µν•λ©΄μ„ μ°¨μ›μ κ°―μλ§νΌμ μ§μ„ μ„ μ°Ύμ

    - **μ£Όμ„±λ¶„ μ„ λ³„**
        - **μ§μ„ **
            - μ„ μ μ°¨λ¥Ό ν†µν•΄ μ°Ύμ€ μ§μ„ λ“¤μ„ ν•΄λ‹Ή λ°μ΄ν„° μ„ΈνΈμ μ£Όμ„±λ¶„(Principle Component; PC)μ΄λΌκ³  μ •μν•¨
        
        - **SS(Sum of Squared Distance)**
            - μ›μ κ³Ό μ‚¬μμ  κ°„ κ±°λ¦¬ μ κ³±μ ν•©μ„ ν•΄λ‹Ή μ§μ„ μ SSλΌκ³  μ •μν•¨
            - μ›μ κ³Ό νΉμ • λ μ½”λ“μ μ‚¬μμ  κ°„ κ±°λ¦¬μ μ κ³±μ„ ν•΄λ‹Ή λ μ½”λ“μ μ£Όμ„±λ¶„κ°’μΌλ΅ ν•΄μ„ν•¨
            - μ „μ²΄ μ§μ„ μ SS λ€λΉ„ νΉμ • μ§μ„ μ SSλ¥Ό ν•΄λ‹Ή μ§μ„ μ΄ μ „μ²΄ νΉμ„±μ„ μ„¤λ…ν•λ” μ •λ„λ΅ ν•΄μ„ν•¨
        
        - **μ£Όμ„±λ¶„ μ„ λ³„**
            - Nμ°¨μ› λ°μ΄ν„° μ„ΈνΈλ¥Ό kμ°¨μ›μΌλ΅ μ¤„μ΄κ³ μ ν•λ” κ²½μ°
            - SS κΈ°μ¤€ μƒμ„ kκ° μ£Όμ„±λ¶„μ„ μ¶”μ¶ν•¨

- **μ£Όμ„±λ¶„ λ¶„μ„μ μν•™μ  μ΄ν•΄**
    
    - **μ£Όμ„±λ¶„ μ¶”μ¶**
        - λ°μ΄ν„° μ„ΈνΈμ κ³µλ¶„μ‚°ν–‰λ ¬μ„ κµ¬ν•¨
        - κ³µλ¶„μ‚°ν–‰λ ¬μ κ³ μ λ²΅ν„°μ™€ κ³ μ κ°’μ„ κµ¬ν•¨
    
    - **μ£Όμ„±λ¶„ μ„ λ³„**
        - **κ³ μ λ²΅ν„°(EigenVector)**
            - μ„ μ μ°¨λ¥Ό ν†µν•΄ μ°Ύμ€ κ³ μ λ²΅ν„°λ¥Ό ν•΄λ‹Ή λ°μ΄ν„° μ„ΈνΈμ μ£Όμ„±λ¶„μ΄λΌκ³  μ •μν•¨
            - νΉμ • λ μ½”λ“μ— λ€μ‘ν•λ” κ³ μ λ²΅ν„°μ μ›μ†λ¥Ό ν•΄λ‹Ή λ μ½”λ“μ μ£Όμ„±λ¶„κ°’μΌλ΅ ν•΄μ„ν•¨
        
        - **κ³ μ κ°’(EigenValue)**
            - μ „μ²΄ κ³ μ λ²΅ν„°μ κ³ μ κ°’ λ€λΉ„ νΉμ • κ³ μ λ²΅ν„°μ κ³ μ κ°’μ„ ν•΄λ‹Ή κ³ μ λ²΅ν„°κ°€ μ „μ²΄ νΉμ„±μ„ μ„¤λ…ν•λ” μ •λ„λ΅ ν•΄μ„ν•¨
        
        - **μ£Όμ„±λ¶„ μ„ λ³„**
            - κ³ μ λ²΅ν„°λ¥Ό κ³ μ κ°’ κΈ°μ¤€μΌλ΅ λ‚΄λ¦Όμ°¨μ μ •λ ¬
            - μ›ν•λ” μ°¨μ› μλ§νΌ κ³ μ λ²΅ν„°λ¥Ό μ„ λ³„

</details>

<details><summary><h3>μ£Όμ” κ°λ…</h3></summary>

- **λ¶„μ‚°(Variance; Var)**

    $$var(X) = \displaystyle\sum_{i=0}^{n}\frac{(X-\overline{X})^2}{n}$$

    - μ •μ : λ‹¨μ°¨μ› λ°μ΄ν„° μ„ΈνΈμ— λ€ν•μ—¬ ν‰κ· μ μ„ μ¤‘μ‹¬μΌλ΅ λ μ½”λ“κ°€ ν©μ–΄μ§„ μ •λ„

- **κ³µλ¶„μ‚°(Covariance; Cov)**

    $$cov(X, Y) = \displaystyle\sum_{i=0}^{n}\frac{(X_i-\overline{X})(Y_i-\overline{Y})}{n}$$

    - μ •μ : λ‹¤μ°¨μ› λ°μ΄ν„° μ„ΈνΈμ— λ€ν•μ—¬ ν‰κ· μ μ„ μ¤‘μ‹¬μΌλ΅ λ μ½”λ“κ°€ ν©μ–΄μ§„ μ •λ„
    - ν•΄μ„ : 2κ°μ μ¶•μ„ κ°€μ •ν–μ„ λ•, ν• λ³€μμ μ¦κ°μ— λ”°λ¥Έ λ‹¤λ¥Έ λ³€μμ μ¦κ° κ²½ν–¥μ„±

- **κ³µλ¶„μ‚°ν–‰λ ¬(Covariance Matrix)**

    $$ \sum = 
    \begin{pmatrix}
    var(X) & cov(X, Y) \\
    cov(Y, X) & var(Y)
    \end{pmatrix} $$

    - **μ •μ**
        - λ‹¤μ°¨μ› λ°μ΄ν„° μ„ΈνΈλ¥Ό κµ¬μ„±ν•λ” λ³€μ(νΉμ€ μ¶•) $X, Y, Z, \cdots$ μ— λ€ν•μ—¬
        - $i$ λ²μ§Έ, $j$ λ²μ§Έ λ³€μ(νΉμ€ μ¶•)μ κ³µλ¶„μ‚°μ„ $(i, j)$ μ κ°’μΌλ΅ κ°€μ§€λ” μ •λ°©ν–‰λ ¬

    - **μƒκ΄€κ΄€κ³„μ™€ κ³µλ¶„μ‚°ν–‰λ ¬**
        - **μƒκ΄€ν–‰λ ¬(Correlation Matrix)** : κ³µλ¶„μ‚°ν–‰λ ¬μ„ μ •κ·ν™”ν• ν–‰λ ¬
        - **ν”Όμ–΄μ¨ μƒκ΄€κ³„μ(Pearson Correlation Coefficient)** : μƒκ΄€ν–‰λ ¬μ„ κµ¬μ„±ν•λ” μ¤μΉΌλΌ

    - **μ„ ν•λ³€ν™κ³Ό κ³µλ¶„μ‚°ν–‰λ ¬**

        ![κ³µλ¶„μ‚°ν–‰λ ¬κ³Ό κ³ μ λ²΅ν„°](https://user-images.githubusercontent.com/116495744/224226188-05975c29-4ac8-4572-b796-fb7eec3bab5a.jpeg)

        - μ„μμ ν–‰λ ¬ Pμ— λ€ν•μ—¬ κ·Έ κ³µλ¶„μ‚°ν–‰λ ¬μ„ ν–‰λ ¬ Qμ— λ‚΄μ ν•λ” κ²½μ°
        - κ·Έλν”„μƒμΌλ΅ ν‘ν„λ Qμ λ¶„ν¬κ°€ Pμ λ¶„ν¬μ™€ μ μ‚¬ν• ν•νƒλ΅ λ³€ν™λ¨

- **κ³ μ λ²΅ν„°(EigenVector)μ™€ κ³ μ κ°’(EigenValue)**

    $$\sum \cdot V = \lambda \times V$$

    - **κ³ μ λ²΅ν„°(EigenVector)** : μ„μμ λ°μ΄ν„° μ„ΈνΈμ— λ€ν•μ—¬ κ·Έ κ³µλ¶„μ‚°ν–‰λ ¬μ„ λ‚΄μ ν•μ—¬ μ„ ν•λ³€ν™ν•λ”λΌλ„ λ°©ν–¥μ΄ λ³€ν™ μ „κ³Ό λ™μΌν• λ²΅ν„°
    - **κ³ μ κ°’(EigenValue)** : μ„μμ λ°μ΄ν„° μ„ΈνΈμ— λ€ν•μ—¬ κ·Έ κ³µλ¶„μ‚°ν–‰λ ¬μ„ λ‚΄μ ν•κΈ° μ „ κ³ μ λ²΅ν„°μ κΈΈμ΄ λ€λΉ„ λ‚΄μ ν• ν›„ κ³ μ λ²΅ν„°μ κΈΈμ΄
    
</details>

<details><summary><h3>SK-Learnμ μ£Όμ„±λ¶„ λ¶„μ„ μ•κ³ λ¦¬μ¦</h3></summary>

- **μ‚¬μ© λ°©λ²•**

    ```
    from sklearn.decomposition import PCA

    # PCA μ•κ³ λ¦¬μ¦ μΈμ¤ν„΄μ¤ μƒμ„±
    # μ¶•μ†ν•  μ°¨μ›μ μλ¥Ό 3μΌλ΅ μ„¤μ •
    pca = PCA(n_components = 3)

    # μ£Όμ„±λ¶„ νƒμƒ‰
    pca.fit(X)

    # λ°μ΄ν„° μ„ΈνΈ μ°¨μ› μ¶•μ†
    X = pca.transform(X)
    ```

- **μ£Όμ” ν•μ΄νΌνλΌλ―Έν„°**
    - `random_state = None`
    - `n_components` : μ¶•μ†ν•  μ°¨μ›μ κ°μ
    - `whiten = False` : μ›μ μ„ λ¨λ“  μ„¤λ…λ³€μλ“¤μ ν‰κ· μΌλ΅ μ΅°μ •ν•  κ²ƒμΈμ§€ μ—¬λ¶€

- **λ‹¤μμ μ†μ„±μ„ ν†µν•΄ ν›λ ¨λ λ¨λΈμ μ •λ³΄λ¥Ό ν™•μΈν•  μ μμ**
    - `n_samples_` : λ μ½”λ“ κ°μ
    - `n_features_` : μ¶•μ† μ „ μ°¨μ›μ κ°μ
    - `feature_names_in_` : μ¶•μ† μ „ μ°¨μ›λ…
    - `mean_` : μ¶•μ† μ „ μ°¨μ›λ³„ ν‰κ· 
    - `n_components_` : μ¶•μ† ν›„ μ°¨μ›μ κ°μ
    - `components_` : κ³ μ λ²΅ν„°
    - `explained_variance_` : κ° κ³ μ λ²΅ν„°μ κ³ μ κ°’
    - `explained_variance_ratio_` : μ „μ²΄ κ³ μ λ²΅ν„°μ κ³ μ κ°’ λ€λΉ„ κ° κ³ μ λ²΅ν„°μ κ³ μ κ°’

</details>

---

## t-SNE(t-distributed Stochastic Neighbor Embedding)

<details><summary><h3>t-SNE λ€ λ¬΄μ—‡μΈκ°€</h3></summary>

- **λ‹¤μ–‘μ²΄ ν•™μµ(Manifold Learning)**

    ![IMG_355193D3C896-1](https://user-images.githubusercontent.com/116495744/224497076-8a2e6100-88a5-444c-abb9-377e61e961ee.jpeg)

    - **μ •μ : λΉ„μ„ ν• μ°¨μ› μ¶•μ† κΈ°λ²•**
        - **λ‹¤μ–‘μ²΄(Manifold)** : λ°μ΄ν„° μ„ΈνΈλ¥Ό κ³ μ°¨μ› κ³µκ°„μ— λ¬μ‚¬ν–μ„ λ•, κ·Έ λ μ½”λ“λ“¤μ„ μ μ•„μ°λ¥Ό μ μλ” μ €μ°¨μ› κ³µκ°„(SubSpace)
        - **λ‹¤μ–‘μ²΄ ν•™μµ(Manifold Learning)** : λ°μ΄ν„° μ„ΈνΈλ¥Ό μ μ•„μ°λ¥Ό μ μλ” λ‹¤μ–‘μ²΄λ¥Ό μ°Ύμ•„ ν•΄λ‹Ή λ°μ΄ν„° μ„ΈνΈμ μ°¨μ›μ„ μ¶•μ†ν•λ” κΈ°λ²•

    - **μΆ…λ¥**
        - t-SNE(t-distributed Stochastic Neighbor Embedding)
        - LLE(Locally Linear Embedding)
        - ISOMAP
        - MDS(Multi-Dimensioning Scaling)
        - AE(Auto Encoder)

- **t-SNE(t-distributed Stochastic Neighbor Embedding)**
    - **μ •μ**
        - κ³ μ°¨μ› κ³µκ°„μ—μ„ μΈμ ‘ν•(Neighbor) λ‘ λ²΅ν„°κ°€ μ €μ°¨μ› κ³µκ°„μ—μ„λ„ μΈμ ‘ν•λ„λ΅ κ³ μ°¨μ›μ—μ„μ μ μ‚¬λ„λ¥Ό λ³΄μ΅΄ν•λ©° μ°¨μ›μ„ μ¶•μ†ν•λ” λ°©λ²•
    
    - **λ°©λ²•**
        - λ°μ΄ν„° μ„ΈνΈλ¥Ό κ³ μ°¨μ› κ³µκ°„μ— λ¬μ‚¬ν•¨
        - λ μ½”λ“ i, jμ— λ€ν•μ—¬, κ³ μ°¨μ› κ³µκ°„μ—μ„ i, j κ°„ κ±°λ¦¬μ κΈ°λ€κ°’ $p$ λ¥Ό κ³„μ‚°ν•¨
        - μ €μ°¨μ› κ³µκ°„μ—μ„ i, j κ°„ κ±°λ¦¬μ κΈ°λ€κ°’ $q$ λ¥Ό κ³„μ‚°ν•¨
        - $p$, $q$ μ μ°¨μ΄λ¥Ό λ°μν•λ” μ†μ‹¤ν•¨μ $C(p, q)$ λ¥Ό μ •μν•¨
        - μ†μ‹¤μ„ μµμ†ν™”ν•λ” μ €μ°¨μ› κ³µκ°„μ„ ν•΄λ‹Ή λ°μ΄ν„° μ„ΈνΈμ λ‹¤μ–‘μ²΄λ΅ μ •μν•¨
        - λ°μ΄ν„° μ„ΈνΈμ μ°¨μ›μ„ λ‹¤μ–‘μ²΄λ΅ λ³€ν™ν•¨

</details>

<details><summary><h3>SK-Learnμ t-SNE μ•κ³ λ¦¬μ¦</h3></summary>

- **μ‚¬μ© λ°©λ²•**

    ```
    from sklearn.manifold import TSNE

    tsne = TSNE()

    X = tsne.fit_transform(X)
    ```

- **μ£Όμ” ν•μ΄νΌνλΌλ―Έν„°**

</details>

---

## π“ Practice

- [**μ‹¤μµ μ½”λ“**]()