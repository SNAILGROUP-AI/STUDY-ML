## π‘¨β€π‘©β€π‘¦ Ensemble

- **μ•™μƒλΈ” κΈ°λ²•μ΄λ€ λ¬΄μ—‡μΈκ°€**
    - λ‹¨μΌ λ¨λΈλ“¤μ„ κ²°ν•©ν•λ” κΈ°λ²•
    - νκ·€ μ•κ³ λ¦¬μ¦μ—λ„ μ•™μƒλΈ” κΈ°λ²•μ„ μ μ©ν•  μ μμΌλ‚, μ£Όλ΅ λ¶„λ¥ μ•κ³ λ¦¬μ¦μ— μ μ©ν•¨

- **μ•™μƒλΈ” κΈ°λ²•μ μΆ…λ¥**
    - **Voting** : μ—¬λ¬ λ¶„λ¥ λ¨λΈλ“¤μ„ μλ™μΌλ΅ κ²°ν•©ν•λ” λ°©μ‹
        - **Hard Voting** : μ„μμ μλ£μ— λ€ν•μ—¬ λ¨λΈλ“¤μ΄ μμΈ΅ν• κ²°κ³Όλ¥Ό μΆ…ν•©ν•μ—¬ λ‹¤μκ²°λ΅ ν•΄λ‹Ή μλ£μ λ²”μ£Όλ¥Ό μ„ μ •ν•λ” λ°©μ‹
        - **Soft Voting** : μ„μμ μλ£μ— λ€ν•μ—¬ λ¨λΈλ“¤μ΄ μμΈ΅ν• λ²”μ£Όλ³„ ν™•λ¥ μ ν‰κ· μ„ λ‚Έ ν›„ ν™•λ¥ μ΄ κ°€μ¥ λ†’μ€ λ²”μ£Όλ¥Ό ν•΄λ‹Ή μλ£μ λ²”μ£Όλ΅ μ„ μ •ν•λ” λ°©μ‹
    
    - **Bagging(Bootstrap Aggregating)** : μ—¬λ¬ λ¶„λ¥ λ¨λΈλ“¤μ„ λ³‘λ ¬λ΅ μ‘μ—…ν•λ„λ΅ μ„¤κ³„ν•λ” λ°©μ‹
    
    - **Boosting** : μ—¬λ¬ λ¶„λ¥ λ¨λΈλ“¤μ„ μ§λ ¬λ΅ μ‹¬ν™” μ‘μ—…ν•λ„λ΅ μ„¤κ³„ν•λ” λ°©μ‹
        - μ²« λ²μ§Έ μ•κ³ λ¦¬μ¦μ„ ν†µν•΄ ν›λ ¨ν•¨
        - μ†μ‹¤(νΉμ€ μ¤μ°¨)μ΄ λ†’μ€ μλ£λ“¤μ— κ°€μ¤‘μΉλ¥Ό λ¶€μ—¬ν•μ—¬ λ‘ λ²μ§Έ μ•κ³ λ¦¬μ¦μΌλ΅ μ „μ†΅ν•¨
        - λ‘ λ²μ§Έ μ•κ³ λ¦¬μ¦μ„ ν†µν•΄ ν›λ ¨ν•λ, μ†μ‹¤μ΄ λ†’μ€ μλ£λ¥Ό μ‹¬ν™” ν›λ ¨ν•¨
        - μ΄μƒμ μ‘μ—…μ„ λ°λ³µν•¨
        - `learning_rate`(ν•™μµλ¥ )μ„ ν†µν•΄ μ–Όλ§λ‚ κ°€μ¤‘ν•  κ²ƒμΈμ§€ μ„¤μ •ν•¨

---

## β… Voting

<details><summary><h3>Example</h3></summary>

- **μ‚¬μ© λ°©λ²•**

    ```
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.ensemble import VotingClassifier
    from sklearn.metrics import accuracy_score

    # κ²°μ • νΈλ¦¬ μ•κ³ λ¦¬μ¦ μΈμ¤ν„΄μ¤ μƒμ„±
    dt_clf = DecisionTreeClassifier()

    # μµκ·Όμ ‘ μ΄μ›ƒ μ•κ³ λ¦¬μ¦ μΈμ¤ν„΄μ¤ μƒμ„±
    knn_clf = KNeighborsClassifier()

    # λ΅μ§€μ¤ν‹± νκ·€ μ•κ³ λ¦¬μ¦ μΈμ¤ν„΄μ¤ μƒμ„±
    lg_clf = LogisticRegression()

    # soft votingμ„ ν†µν•΄ μ„Έ μΈμ¤ν„΄μ¤λ¥Ό κ²°ν•©ν•μ—¬ μ•™μƒλΈ” μ•κ³ λ¦¬μ¦ μΈμ¤ν„΄μ¤ μƒμ„±
    voting_clf = VotingClassifier(
        estimators = [('DT', dt_clf), ('KNN', knn_clf), ('LG', lg_clf)],
        voting = 'soft'
        )

    # ν›λ ¨μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ μΈμ¤ν„΄μ¤λ¥Ό ν›λ ¨μ‹μΌμ„ λ¨λΈ μ„¤κ³„
    voting_clf.fit(X_train, y_train)

    # ν‰κ°€μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ μμΈ΅
    y_predict = voting_clf.predict(X_test)

    # λ€ν‘μ μΈ μ„±λ¥ ν‰κ°€ μ§€ν‘μΈ κ²°μ •κ³„μλ¥Ό ν†µν•΄ μ„±λ¥ ν‰κ°€
    score = accuracy_score(y_test, y_predict)
    print(score)
    ```

- **μ£Όμ” ν•μ΄νΌνλΌλ―Έν„°**
    - `estimators` : λ™μ›ν•  μ•κ³ λ¦¬μ¦ μΈμ¤ν„΄μ¤ λ¦¬μ¤νΈ
    
    - `voting = hard` : μ„¤κ³„ λ°©μ‹
        - `hard` : hard voting
        - `soft` : soft voting

</details>

---

## π¤ Bagging

<details><summary><h3>Random Forest</h3></summary>

- **μ •μ**

- **μ‚¬μ© λ°©λ²•**

    ```

    ```

- **μ£Όμ” ν•μ΄νΌνλΌλ―Έν„°**

</details>

---

## π•µοΈ Boosting

<details><summary><h3>Gradient Boosting Machine(GBM)</h3></summary>

- **μ •μ**

- **μ‚¬μ© λ°©λ²•**

    ```
    from sklearn.ensemble import GradientBoostingClassifier
    from sklearn.metrics import accuracy_score
    
    # GBM μ•κ³ λ¦¬μ¦ μΈμ¤ν„΄μ¤ μƒμ„±
    gb_clf = GradientBoostingClassifier()

    # ν›λ ¨μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ μΈμ¤ν„΄μ¤λ¥Ό ν›λ ¨μ‹μΌμ„ λ¨λΈ μ„¤κ³„
    gb_clf.fit(X_train, y_train)
    
    # ν‰κ°€μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ μμΈ΅
    y_predict = gb_clf.predict(X_test)
    
    # λ€ν‘μ μΈ μ„±λ¥ ν‰κ°€ μ§€ν‘μΈ κ²°μ •κ³„μλ¥Ό ν†µν•΄ μ„±λ¥ ν‰κ°€
    socre = accuracy_score(y_test, y_predict)
    print(score)
    ```

- **μ£Όμ” ν•μ΄νΌνλΌλ―Έν„°**

</details>

<details><summary><h3>eXtra Gradient Boosting(XGBoost)</h3></summary>

- **μ •μ**
    - λ³‘λ ¬ μ²λ¦¬κ°€ λ¶κ°€λ¥ν•μ—¬ μ†λ„κ°€ λλ¦¬κ³  κ³Όμ ν•©μ΄ λ°μƒν•  μ°λ ¤κ°€ μλ” GBM μ•κ³ λ¦¬μ¦μ„ λ³΄μ™„ν•¨
    - λ‚΄μ¥λ κµμ°¨κ²€μ¦ μ μ°¨λ¥Ό ν†µν•΄ μμΈ΅ μ„±λ¥μ΄ ν–¥μƒλμ§€ μ•μ„ κ²½μ° Early Stoppingν•  μ μμ

- **μ‚¬μ© λ°©λ²•**

    ```
    from xgboost import XGBClassifier
    from sklearn.metrics import accuracy_score
    
    # XGB μ•κ³ λ¦¬μ¦ μΈμ¤ν„΄μ¤ μƒμ„±
    xgb_clf = XGBClassifier()
    
    # κµμ°¨κ²€μ¦ μ‹ μ‚¬μ©ν•  λ°μ΄ν„° μ„ΈνΈ κµ¬μ„±
    evals = [(X_val, y_val)]

    # ν›λ ¨μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ μΈμ¤ν„΄μ¤λ¥Ό ν›λ ¨μ‹μΌμ„ λ¨λΈ μ„¤κ³„
    # κ²€μ¦μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ κµμ°¨κ²€μ¦
    xgb_clf.fit(
        X_train, y_train,
        eval_set = evals,
        eval_metric = 'logloss'
        )

    # ν‰κ°€μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ μμΈ΅
    y_predict = xgb_clf.predict(X_test)
    
    # λ€ν‘μ μΈ μ„±λ¥ ν‰κ°€ μ§€ν‘μΈ κ²°μ •κ³„μλ¥Ό ν†µν•΄ μ„±λ¥ ν‰κ°€
    socre = accuracy_score(y_test, y_predict)
    print(score)
    ```

- **μ£Όμ” ν•μ΄νΌνλΌλ―Έν„°**
    - **μΈμ¤ν„΄μ¤ μƒμ„± μ‹ μ„¤μ •**
        - `random_state = None`
        - `n_estimators = 100` : λ™μ›ν•  λ¨λΈμ κ°μ
        - `learning_rate = 0.1` : ν•™μµλ¥ 
        - `max_depth = -1` : νΈλ¦¬ μµλ€ κΉμ΄
        - `num_leaves = 31` : ν•λ‚μ νΈλ¦¬κ°€ μµλ€λ΅ κ°€μ§ μ μλ” leaf_nodeμ κ°μ
        - `min_child_samples = 20` : leaf_nodeκ°€ λκΈ° μ„ν•΄ ν•„μ”ν• μµμ†ν•μ μƒν” κ°μ

    - **ν›λ ¨ μ‹ μ„¤μ •**
        - `early_stopping_rounds = None` : ν•™μµμ΄ μ¥κΈ°ν™”λ  κ²½μ° μ΅°κΈ° μΆ…λ£ν•κΈ° μ„ν• μ΅°κ±΄μΌλ΅μ„ μµλ€ ν•™μµ νμ
        
        - `eval_set` : μ„±λ¥ κµμ°¨κ²€μ¦μ— μ‚¬μ©ν•  λ°μ΄ν„° μ„ΈνΈ
            - λ°μ΄ν„° μ„ΈνΈ λ¶„ν•  μ‹ μ°μ„  ν›λ ¨μ©κ³Ό ν‰κ°€μ©μΌλ΅ λ¶„ν• ν•¨
            - μ΄ν›„ ν›λ ¨μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν›λ ¨μ©κ³Ό κ²€μ¦μ©μΌλ΅ μ¬λ¶„ν• ν•¨
        
        - `eval_metric` : κµμ°¨κ²€μ¦ μ‹ μ‚¬μ©ν•  ν‰κ°€ μ§€ν‘
            - `logloss` : μ΄ν•­λ¶„λ¥λ¶„μ„ κµμ°¨κ²€μ¦ μ‹ ν‰κ°€ μ§€ν‘
            - `multi-logloss` : λ‹¤ν•­λ¶„λ¥λ¶„μ„ κµμ°¨κ²€μ¦ μ‹ ν‰κ°€ μ§€ν‘

- **λ‹¤μμ„ ν†µν•΄ ν•™μµλ λ¨λΈμ΄ κ³„μ‚°ν• μ„¤λ…λ³€μλ³„ κ°€μ¤‘μΉλ¥Ό μ‹κ°ν™”ν•  μ μμ**

    ```
    from xgboost import plot_importance
    print(plot_importance(xgb_clf))
    ```

</details>

<details><summary><h3>Light Gradient Boosting Machine(LightGBM)</h3></summary>

- **μ •μ**

- **μ‚¬μ© λ°©λ²•**

    ```
    from lightgbm import LGBMClassifier
    from sklearn.metrics import accuracy_score
    
    # Light GBM μ•κ³ λ¦¬μ¦ μΈμ¤ν„΄μ¤ μƒμ„±
    lgb_clf = LGBMClassifier()
    
    # κµμ°¨κ²€μ¦ μ‹ μ‚¬μ©ν•  λ°μ΄ν„° μ„ΈνΈ κµ¬μ„±
    evals = [(X_val, y_val)]

    # ν›λ ¨μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ μΈμ¤ν„΄μ¤λ¥Ό ν›λ ¨μ‹μΌμ„ λ¨λΈ μ„¤κ³„
    # κ²€μ¦μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ κµμ°¨κ²€μ¦
    lgb_clf.fit(
        X_train, y_train,
        eval_set = evals,
        eval_metric = 'logloss'
        )

    # ν‰κ°€μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ μμΈ΅
    y_predict = lgb_clf.predict(X_test)
    
    # λ€ν‘μ μΈ μ„±λ¥ ν‰κ°€ μ§€ν‘μΈ κ²°μ •κ³„μλ¥Ό ν†µν•΄ μ„±λ¥ ν‰κ°€
    socre = accuracy_score(y_test, y_predict)
    print(score)
    ``

- **μ£Όμ” ν•μ΄νΌνλΌλ―Έν„°**
    - **μΈμ¤ν„΄μ¤ μƒμ„± μ‹ μ„¤μ •**
        - `random_state = None`
        - `n_estimators = 100` : λ™μ›ν•  λ¨λΈμ κ°μ
        - `learning_rate = 0.1` : ν•™μµλ¥ 
        - `max_depth = -1` : νΈλ¦¬ μµλ€ κΉμ΄
        - `num_leaves = 31` : ν•λ‚μ νΈλ¦¬κ°€ μµλ€λ΅ κ°€μ§ μ μλ” leaf_nodeμ κ°μ
        - `min_child_samples = 20` : leaf_nodeκ°€ λκΈ° μ„ν•΄ ν•„μ”ν• μµμ†ν•μ μƒν” κ°μ

    - **ν›λ ¨ μ‹ μ„¤μ •**
        - `early_stopping_rounds = None` : ν•™μµμ΄ μ¥κΈ°ν™”λ  κ²½μ° μ΅°κΈ° μΆ…λ£ν•κΈ° μ„ν• μ΅°κ±΄μΌλ΅μ„ μµλ€ ν•™μµ νμ
        
        - `eval_set` : μ„±λ¥ κµμ°¨κ²€μ¦μ— μ‚¬μ©ν•  λ°μ΄ν„° μ„ΈνΈ
            - λ°μ΄ν„° μ„ΈνΈ λ¶„ν•  μ‹ μ°μ„  ν›λ ¨μ©κ³Ό ν‰κ°€μ©μΌλ΅ λ¶„ν• ν•¨
            - μ΄ν›„ ν›λ ¨μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν›λ ¨μ©κ³Ό κ²€μ¦μ©μΌλ΅ μ¬λ¶„ν• ν•¨
        
        - `eval_metric` : κµμ°¨κ²€μ¦ μ‹ μ‚¬μ©ν•  ν‰κ°€ μ§€ν‘
            - `logloss` : μ΄ν•­λ¶„λ¥λ¶„μ„ κµμ°¨κ²€μ¦ μ‹ ν‰κ°€ μ§€ν‘
            - `multi-logloss` : λ‹¤ν•­λ¶„λ¥λ¶„μ„ κµμ°¨κ²€μ¦ μ‹ ν‰κ°€ μ§€ν‘

</details>