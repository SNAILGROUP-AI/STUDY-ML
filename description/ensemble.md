<img src="https://capsule-render.vercel.app/api?type=cylinder&color=auto&fontColor=000000&stroke=FFFFFF&strokeWidth=1&height=100&section=header&text=Ensemble&fontSize=40" width=100%/>

</br>

## π‘¨β€π‘©β€π‘¦ Ensemble

- **μ•™μƒλΈ” κΈ°λ²•μ΄λ€ λ¬΄μ—‡μΈκ°€**
    - λ‹¨μΌ λ¨λΈλ“¤μ„ κ²°ν•©ν•λ” κΈ°λ²•
    - νκ·€ μ•κ³ λ¦¬μ¦μ—λ„ μ•™μƒλΈ” κΈ°λ²•μ„ μ μ©ν•  μ μμΌλ‚, μ£Όλ΅ λ¶„λ¥ μ•κ³ λ¦¬μ¦μ— μ μ©ν•¨

- **μ•™μƒλΈ” κΈ°λ²•μ μΆ…λ¥**
    - **Voting** : μ—¬λ¬ λ¶„λ¥ λ¨λΈλ“¤μ„ μλ™μΌλ΅ κ²°ν•©ν•λ” λ°©μ‹
        - **Hard Voting** : μ„μμ μλ£μ— λ€ν•μ—¬ λ¨λΈλ“¤μ΄ μμΈ΅ν• κ²°κ³Όλ¥Ό μΆ…ν•©ν•μ—¬ λ‹¤μκ²°λ΅ ν•΄λ‹Ή μλ£μ λ²”μ£Όλ¥Ό μ„ μ •ν•λ” λ°©μ‹
        - **Soft Voting** : μ„μμ μλ£μ— λ€ν•μ—¬ λ¨λΈλ“¤μ΄ μμΈ΅ν• λ²”μ£Όλ³„ ν™•λ¥ μ ν‰κ· μ„ λ‚Έ ν›„ ν™•λ¥ μ΄ κ°€μ¥ λ†’μ€ λ²”μ£Όλ¥Ό ν•΄λ‹Ή μλ£μ λ²”μ£Όλ΅ μ„ μ •ν•λ” λ°©μ‹
    
    - **Bagging(Bootstrap Aggregating)** : μ—¬λ¬ λ¶„λ¥ λ¨λΈλ“¤μ„ λ³‘λ ¬λ΅ μ‘μ—…ν•λ„λ΅ μ„¤κ³„ν•λ” λ°©μ‹
    
    - **Boosting** : μ—¬λ¬ λ¶„λ¥ λ¨λΈλ“¤μ„ μ§λ ¬λ΅ μ‹¬ν™” μ‘μ—…ν•λ„λ΅ μ„¤κ³„ν•λ” λ°©μ‹
        - μ²« λ²μ§Έ μ•κ³ λ¦¬μ¦μ„ ν†µν•΄ ν›λ ¨ν•¨
        - μ†μ‹¤(νΉμ€ μ¤μ°¨)μ΄ λ†’μ€ μλ£λ“¤μ— κ°€μ¤‘μΉλ¥Ό λ¶€μ—¬ν•μ—¬ λ‘ λ²μ§Έ μ•κ³ λ¦¬μ¦μΌλ΅ μ „μ†΅ν•¨
        - λ‘ λ²μ§Έ μ•κ³ λ¦¬μ¦μ„ ν†µν•΄ ν›λ ¨ν•λ, μ†μ‹¤μ΄ λ†’μ€ μλ£λ¥Ό μ‹¬ν™” ν›λ ¨ν•¨
        - μ΄μƒμ μ‘μ—…μ„ λ°λ³µν•¨
        - `learning_rate`(ν•™μµλ¥ )μ„ ν†µν•΄ μ–Όλ§λ‚ κ°€μ¤‘ν•  κ²ƒμΈμ§€ μ„¤μ •ν•¨

---

## β… Voting

<details><summary><h3>Example</h3></summary>

- **μ‚¬μ© λ°©λ²•**

    ```
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.ensemble import VotingClassifier
    from sklearn.metrics import accuracy_score

    # κ²°μ • νΈλ¦¬ μ•κ³ λ¦¬μ¦ μΈμ¤ν„΄μ¤ μƒμ„±
    dt_clf = DecisionTreeClassifier()

    # μµκ·Όμ ‘ μ΄μ›ƒ μ•κ³ λ¦¬μ¦ μΈμ¤ν„΄μ¤ μƒμ„±
    knn_clf = KNeighborsClassifier()

    # λ΅μ§€μ¤ν‹± νκ·€ μ•κ³ λ¦¬μ¦ μΈμ¤ν„΄μ¤ μƒμ„±
    lg_clf = LogisticRegression()

    # soft votingμ„ ν†µν•΄ μ„Έ μΈμ¤ν„΄μ¤λ¥Ό κ²°ν•©ν•μ—¬ μ•™μƒλΈ” μ•κ³ λ¦¬μ¦ μΈμ¤ν„΄μ¤ μƒμ„±
    voting_clf = VotingClassifier(
        estimators = [('DT', dt_clf), ('KNN', knn_clf), ('LG', lg_clf)],
        voting = 'soft'
        )

    # ν›λ ¨μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ μΈμ¤ν„΄μ¤λ¥Ό ν›λ ¨μ‹μΌμ„ λ¨λΈ μ„¤κ³„
    voting_clf.fit(X_train, y_train)

    # ν‰κ°€μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ μμΈ΅
    y_predict = voting_clf.predict(X_test)

    # λ€ν‘μ μΈ μ„±λ¥ ν‰κ°€ μ§€ν‘μΈ κ²°μ •κ³„μλ¥Ό ν†µν•΄ μ„±λ¥ ν‰κ°€
    score = accuracy_score(y_test, y_predict)
    print(score)
    ```

- **μ£Όμ” ν•μ΄νΌνλΌλ―Έν„°**
    - `estimators` : λ™μ›ν•  μ•κ³ λ¦¬μ¦ μΈμ¤ν„΄μ¤ λ¦¬μ¤νΈ
    
    - `voting = 'hard'` : μ„¤κ³„ λ°©μ‹
        - `hard` : hard voting
        - `soft` : soft voting

</details>

---

## π¤ Bagging

<details><summary><h3>Random Forest</h3></summary>

- **μ •μ : λ€ν‘μ μΈ Bagging λ°©μ‹μ μ•κ³ λ¦¬μ¦**
    - κ²°μ • νΈλ¦¬ μ•κ³ λ¦¬μ¦μ— κΈ°λ°ν•¨

- **μ‚¬μ© λ°©λ²•**

    ```
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import accuracy_score

    # λλ¤ ν¬λ μ¤νΈ μ•κ³ λ¦¬μ¦ μΈμ¤ν„΄μ¤ μƒμ„±
    rf_clf = RandomForestClassifier()

    # ν›λ ¨μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ μΈμ¤ν„΄μ¤λ¥Ό ν›λ ¨μ‹μΌμ„ λ¨λΈ μ„¤κ³„
    rf_clf.fit(X_train, y_train)
    
    # ν‰κ°€μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ μμΈ΅
    y_predict = rf_clf.predict(X_test)
    
    # λ€ν‘μ μΈ μ„±λ¥ ν‰κ°€ μ§€ν‘μΈ κ²°μ •κ³„μλ¥Ό ν†µν•΄ μ„±λ¥ ν‰κ°€
    socre = accuracy_score(y_test, y_predict)
    print(score)
    ```

- **μ£Όμ” ν•μ΄νΌνλΌλ―Έν„°**
    - `random_state = None`
    - `n_estimators = 100` : λ™μ›ν•  λ¨λΈμ κ°μ
    - `max_depth = -1` : νΈλ¦¬ μµλ€ κΉμ΄
    - `min_samples_split = 2` : ν•μ„ λ…Έλ“λ΅ κ°€μ§€μΉκΈ°ν•κΈ° μ„ν•΄ ν•„μ”ν• μµμ†ν•μ μƒν” κ°μ
    - `min_samples_leaf = 1` : λ¦¬ν”„ λ…Έλ“κ°€ λκΈ° μ„ν•΄ ν•„μ”ν• μµμ†ν•μ μƒν” κ°μ

- **λ‹¤μμ μ†μ„±μ„ ν†µν•΄ ν›λ ¨λ λ¨λΈμ μ •λ³΄λ¥Ό ν™•μΈν•  μ μμ**
    - `feature_importances_` : μ„¤λ…λ³€μλ³„ κ°€μ¤‘μΉ

- **κΈ°νƒ€ ν•μ΄νΌνλΌλ―Έν„° λ° λ¨λΈ μ •λ³΄ λ©λ΅μ€ `estimator_params`μ„ ν†µν•΄ ν™•μΈ κ°€λ¥ν•¨**

</details>

---

## π•µοΈ Boosting

<details><summary><h3>Gradient Boosting Machine(GBM)</h3></summary>

- **μ •μ : λ€ν‘μ μΈ Boosting λ°©μ‹μ μ•κ³ λ¦¬μ¦**
    - κ²°μ • νΈλ¦¬ μ•κ³ λ¦¬μ¦μ— κΈ°λ°ν•¨
    - μ†μ‹¤ μΈ΅μ • λ° μµμ†ν™” λ°©λ²•μΌλ΅μ„ κ²½μ‚¬ν•κ°•λ²•μ„ μ±„νƒν•¨

- **μ‚¬μ© λ°©λ²•**

    ```
    from sklearn.ensemble import GradientBoostingClassifier
    from sklearn.metrics import accuracy_score
    
    # GBM μ•κ³ λ¦¬μ¦ μΈμ¤ν„΄μ¤ μƒμ„±
    gbm_clf = GradientBoostingClassifier()

    # ν›λ ¨μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ μΈμ¤ν„΄μ¤λ¥Ό ν›λ ¨μ‹μΌμ„ λ¨λΈ μ„¤κ³„
    gbm_clf.fit(X_train, y_train)
    
    # ν‰κ°€μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ μμΈ΅
    y_predict = gbm_clf.predict(X_test)
    
    # λ€ν‘μ μΈ μ„±λ¥ ν‰κ°€ μ§€ν‘μΈ κ²°μ •κ³„μλ¥Ό ν†µν•΄ μ„±λ¥ ν‰κ°€
    socre = accuracy_score(y_test, y_predict)
    print(score)
    ```

- **μ£Όμ” ν•μ΄νΌνλΌλ―Έν„°**
    - `random_state = None`
    - `n_estimators = 100` : λ™μ›ν•  λ¨λΈμ κ°μ
    - `learning_rate = 0.1` : ν•™μµλ¥ 
    - `max_depth = -1` : νΈλ¦¬ μµλ€ κΉμ΄
    - `min_samples_split = 2` : ν•μ„ λ…Έλ“λ΅ κ°€μ§€μΉκΈ°ν•κΈ° μ„ν•΄ ν•„μ”ν• μµμ†ν•μ μƒν” κ°μ
    - `min_samples_leaf = 1` : λ¦¬ν”„ λ…Έλ“κ°€ λκΈ° μ„ν•΄ ν•„μ”ν• μµμ†ν•μ μƒν” κ°μ

- **λ‹¤μμ μ†μ„±μ„ ν†µν•΄ ν›λ ¨λ λ¨λΈμ μ •λ³΄λ¥Ό ν™•μΈν•  μ μμ**
    - `feature_importances_` : μ„¤λ…λ³€μλ³„ κ°€μ¤‘μΉ

- **κΈ°νƒ€ ν•μ΄νΌνλΌλ―Έν„° λ° λ¨λΈ μ •λ³΄ λ©λ΅μ€ `estimator_params`μ„ ν†µν•΄ ν™•μΈ κ°€λ¥ν•¨**

</details>

<details><summary><h3>eXtra Gradient Boosting(XGBoost)</h3></summary>

- **μ •μ : GBMμ„ κ· ν• νΈλ¦¬ λ¶„ν•  λ°©μ‹μ„ λ³΄μ΅΄ν•λ©΄μ„ λ³΄μ™„ν• μ•κ³ λ¦¬μ¦**
    - λ³‘λ ¬ μ²λ¦¬κ°€ λ¶κ°€λ¥ν•μ—¬ μ†λ„κ°€ λλ¦¬κ³  κ³Όμ ν•©μ΄ λ°μƒν•  μ°λ ¤κ°€ μλ” GBM μ•κ³ λ¦¬μ¦μ„ λ³΄μ™„ν•¨
    - λ‚΄μ¥λ κµμ°¨κ²€μ¦ μ μ°¨λ¥Ό ν†µν•΄ μμΈ΅ μ„±λ¥μ΄ ν–¥μƒλμ§€ μ•μ„ κ²½μ° Early Stoppingν•  μ μμ

- **κ· ν• νΈλ¦¬ λ¶„ν•  λ°©μ‹ : λ¨λ“  λ¦¬ν”„ λ…Έλ“μ—μ„ κ· λ“±ν•κ² κ°€μ§€μΉκΈ°ν•λ” λ°©μ‹**
    ![νΈλ¦¬ λ¶„ν•  λ°©μ‹](https://www.mdpi.com/mathematics/mathematics-08-00765/article_deploy/html/images/mathematics-08-00765-g005-550.jpg)
  
    - λ¦¬ν”„ μ¤‘μ‹¬ νΈλ¦¬ λ¶„ν•  λ°©μ‹λ³΄λ‹¤ κ³Όμ ν•© λ°©μ§€μ— λ›°μ–΄λ‚¨
        - νΈλ¦¬ ν•νƒκ°€ λΉ„λ€μΉ­μ μΌλ΅ ν•μ„±λλ” κ²ƒμ„ λ°©μ§€ν•  μ μμ
        - νΈλ¦¬ κΉμ΄κ°€ μ‹¬ν™”λλ” κ²ƒμ„ λ°©μ§€ν•  μ μμ
    
    - λ¦¬ν”„ μ¤‘μ‹¬ λ¶„ν•  λ°©μ‹μ— λΉ„ν•΄ μ†μ‹¤μ„ μµμ†ν™”ν•μ§€ λ»ν•  κ°€λ¥μ„±μ΄ λ†’μ
    
    - κ²°μ • νΈλ¦¬λ¥Ό κΈ°λ°μΌλ΅ ν•λ” λ€λ¶€λ¶„μ μ•κ³ λ¦¬μ¦μ€ κ· ν• νΈλ¦¬ λ¶„ν•  λ°©μ‹μ„ μ±„νƒν•¨

- **μ‚¬μ© λ°©λ²•**

    ```
    from xgboost import XGBClassifier
    from sklearn.metrics import accuracy_score
    
    # XGB μ•κ³ λ¦¬μ¦ μΈμ¤ν„΄μ¤ μƒμ„±
    xgb_clf = XGBClassifier()
    
    # κµμ°¨κ²€μ¦ μ‹ μ‚¬μ©ν•  λ°μ΄ν„° μ„ΈνΈ κµ¬μ„±
    evals = [(X_val, y_val)]

    # ν›λ ¨μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ μΈμ¤ν„΄μ¤λ¥Ό ν›λ ¨μ‹μΌμ„ λ¨λΈ μ„¤κ³„
    # κ²€μ¦μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ κµμ°¨κ²€μ¦
    xgb_clf.fit(
        X_train, y_train,
        eval_set = evals,
        eval_metric = 'logloss'
        )

    # ν‰κ°€μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ μμΈ΅
    y_predict = xgb_clf.predict(X_test)
    
    # λ€ν‘μ μΈ μ„±λ¥ ν‰κ°€ μ§€ν‘μΈ κ²°μ •κ³„μλ¥Ό ν†µν•΄ μ„±λ¥ ν‰κ°€
    socre = accuracy_score(y_test, y_predict)
    print(score)
    ```

- **μ£Όμ” ν•μ΄νΌνλΌλ―Έν„°**
    - **μΈμ¤ν„΄μ¤ μƒμ„± μ‹ μ„¤μ •**
        - `random_state = None`
        - `n_estimators = 100` : λ™μ›ν•  λ¨λΈμ κ°μ
        - `learning_rate = 0.1` : ν•™μµλ¥ 
        - `max_depth = 6` : νΈλ¦¬ μµλ€ κΉμ΄
        - `min_child_weight = 1` : ν•μ„ λ…Έλ“λ΅ κ°€μ§€μΉκΈ°ν•κΈ° μ„ν•΄ ν•„μ”ν• μµμ†ν•μ μƒν” κ°μ

    - **ν›λ ¨ μ‹ μ„¤μ •**
        - `early_stopping_rounds = None` : ν•™μµμ΄ μ¥κΈ°ν™”λ  κ²½μ° μ΅°κΈ° μΆ…λ£ν•κΈ° μ„ν• μ΅°κ±΄μΌλ΅μ„ μµλ€ ν•™μµ νμ
        
        - `eval_set` : μ„±λ¥ κµμ°¨κ²€μ¦μ— μ‚¬μ©ν•  λ°μ΄ν„° μ„ΈνΈ
            - λ°μ΄ν„° μ„ΈνΈ λ¶„ν•  μ‹ μ°μ„  ν›λ ¨μ©κ³Ό ν‰κ°€μ©μΌλ΅ λ¶„ν• ν•¨
            - μ΄ν›„ ν›λ ¨μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν›λ ¨μ©κ³Ό κ²€μ¦μ©μΌλ΅ μ¬λ¶„ν• ν•¨
            - κ²€μ¦μ© λ°μ΄ν„° μ„ΈνΈλ΅ ν•λ‚ μ΄μƒμ„ μ‚¬μ©ν•  μ μμ
        
        - `eval_metric` : κµμ°¨κ²€μ¦ μ‹ μ‚¬μ©ν•  ν‰κ°€ μ§€ν‘
            - `logloss` : μ΄ν•­λ¶„λ¥λ¶„μ„ κµμ°¨κ²€μ¦ μ‹ ν‰κ°€ μ§€ν‘
            - `multi-logloss` : λ‹¤ν•­λ¶„λ¥λ¶„μ„ κµμ°¨κ²€μ¦ μ‹ ν‰κ°€ μ§€ν‘

- **λ‹¤μμ μ†μ„±μ„ ν†µν•΄ ν›λ ¨λ λ¨λΈμ μ •λ³΄λ¥Ό ν™•μΈν•  μ μμ**
    - `feature_importances_` : μ„¤λ…λ³€μλ³„ κ°€μ¤‘μΉ

- **λ‹¤μμ„ ν†µν•΄ ν•™μµλ λ¨λΈμ΄ κ³„μ‚°ν• μ„¤λ…λ³€μλ³„ κ°€μ¤‘μΉλ¥Ό μ‹κ°ν™”ν•  μ μμ**

    ```
    from xgboost import plot_importance
    print(plot_importance(xgb_clf))
    ```

- **κΈ°νƒ€ ν•μ΄νΌνλΌλ―Έν„° λ° λ¨λΈ μ •λ³΄ λ©λ΅μ€ `estimator_params`μ„ ν†µν•΄ ν™•μΈ κ°€λ¥ν•¨**

</details>

<details><summary><h3>Light Gradient Boosting Machine(LightGBM)</h3></summary>

- **μ •μ : GBMμ„ λ¦¬ν”„ μ¤‘μ‹¬ νΈλ¦¬ λ¶„ν•  λ°©μ‹μΌλ΅ λ³΄μ™„ν• μ•κ³ λ¦¬μ¦**
    - λ³‘λ ¬ μ²λ¦¬κ°€ λ¶κ°€λ¥ν•μ—¬ μ†λ„κ°€ λλ¦¬κ³  κ³Όμ ν•©μ΄ λ°μƒν•  μ°λ ¤κ°€ μλ” GBM μ•κ³ λ¦¬μ¦μ„ λ³΄μ™„ν•¨
    - λ‚΄μ¥λ κµμ°¨κ²€μ¦ μ μ°¨λ¥Ό ν†µν•΄ μμΈ΅ μ„±λ¥μ΄ ν–¥μƒλμ§€ μ•μ„ κ²½μ° Early Stoppingν•  μ μμ

- **λ¦¬ν”„ μ¤‘μ‹¬ νΈλ¦¬ λ¶„ν•  λ°©μ‹ : λ¦¬ν”„ λ…Έλ“ μ¤‘ μ†μ‹¤μ΄ κ°€μ¥ ν° λ…Έλ“λ§ μ·¨μ‚¬μ„ νƒν•μ—¬ λ¶„ν• ν•λ” λ°©μ‹**

    ![νΈλ¦¬ λ¶„ν•  λ°©μ‹](https://www.mdpi.com/mathematics/mathematics-08-00765/article_deploy/html/images/mathematics-08-00765-g005-550.jpg)

    - κ· ν• νΈλ¦¬ λ¶„ν•  λ°©μ‹λ³΄λ‹¤ μ†μ‹¤ μµμ†ν™”μ— λ›°μ–΄λ‚¨
        - λ¦¬ν”„ λ…Έλ“ μ¤‘ μ†μ‹¤μ΄ ν° λ…Έλ“λ¥Ό μ‹¬ν™” μ‘μ—…ν•¨
        
    - κ· ν• νΈλ¦¬ λ¶„ν•  λ°©μ‹μ— λΉ„ν•΄ κ³Όμ ν•© λ°μƒ κ°€λ¥μ„±μ΄ λ†’μ
        - νΈλ¦¬ ν•νƒκ°€ λΉ„λ€μΉ­μ μΌλ΅ ν•μ„±λ¨
        - νΈλ¦¬ κΉμ΄κ°€ μ‹¬ν™”λ  μ μμ
        
    - ν•™μµν•  λ°μ΄ν„°μ μκ°€ μ μ„ κ²½μ° μ§€μ–‘ν•  κ²ƒμ„ κ¶μ¥ν•¨

- **μ‚¬μ© λ°©λ²•**

    ```
    from lightgbm import LGBMClassifier
    from sklearn.metrics import accuracy_score
    
    # Light GBM μ•κ³ λ¦¬μ¦ μΈμ¤ν„΄μ¤ μƒμ„±
    lgb_clf = LGBMClassifier()
    
    # κµμ°¨κ²€μ¦ μ‹ μ‚¬μ©ν•  λ°μ΄ν„° μ„ΈνΈ κµ¬μ„±
    evals = [(X_val, y_val)]

    # ν›λ ¨μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ μΈμ¤ν„΄μ¤λ¥Ό ν›λ ¨μ‹μΌμ„ λ¨λΈ μ„¤κ³„
    # κ²€μ¦μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ κµμ°¨κ²€μ¦
    lgb_clf.fit(
        X_train, y_train,
        eval_set = evals,
        eval_metric = 'logloss'
        )

    # ν‰κ°€μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν†µν•΄ μμΈ΅
    y_predict = lgb_clf.predict(X_test)
    
    # λ€ν‘μ μΈ μ„±λ¥ ν‰κ°€ μ§€ν‘μΈ κ²°μ •κ³„μλ¥Ό ν†µν•΄ μ„±λ¥ ν‰κ°€
    socre = accuracy_score(y_test, y_predict)
    print(score)
    ```

- **μ£Όμ” ν•μ΄νΌνλΌλ―Έν„°**
    - **μΈμ¤ν„΄μ¤ μƒμ„± μ‹ μ„¤μ •**
        - `random_state = None`
        - `n_estimators = 100` : λ™μ›ν•  λ¨λΈμ κ°μ
        - `learning_rate = 0.1` : ν•™μµλ¥ 
        - `max_depth = -1` : νΈλ¦¬ μµλ€ κΉμ΄
        - `min_child_samples = 20` : λ¦¬ν”„ λ…Έλ“κ°€ λκΈ° μ„ν•΄ ν•„μ”ν• μµμ†ν•μ μƒν” κ°μ

    - **ν›λ ¨ μ‹ μ„¤μ •**
        - `early_stopping_rounds = None` : ν•™μµμ΄ μ¥κΈ°ν™”λ  κ²½μ° μ΅°κΈ° μΆ…λ£ν•κΈ° μ„ν• μ΅°κ±΄μΌλ΅μ„ μµλ€ ν•™μµ νμ
        
        - `eval_set` : μ„±λ¥ κµμ°¨κ²€μ¦μ— μ‚¬μ©ν•  λ°μ΄ν„° μ„ΈνΈ
            - λ°μ΄ν„° μ„ΈνΈ λ¶„ν•  μ‹ μ°μ„  ν›λ ¨μ©κ³Ό ν‰κ°€μ©μΌλ΅ λ¶„ν• ν•¨
            - μ΄ν›„ ν›λ ¨μ© λ°μ΄ν„° μ„ΈνΈλ¥Ό ν›λ ¨μ©κ³Ό κ²€μ¦μ©μΌλ΅ μ¬λ¶„ν• ν•¨
        
        - `eval_metric` : κµμ°¨κ²€μ¦ μ‹ μ‚¬μ©ν•  ν‰κ°€ μ§€ν‘
            - `logloss` : μ΄ν•­λ¶„λ¥λ¶„μ„ κµμ°¨κ²€μ¦ μ‹ ν‰κ°€ μ§€ν‘
            - `multi-logloss` : λ‹¤ν•­λ¶„λ¥λ¶„μ„ κµμ°¨κ²€μ¦ μ‹ ν‰κ°€ μ§€ν‘

- **λ‹¤μμ μ†μ„±μ„ ν†µν•΄ ν›λ ¨λ λ¨λΈμ μ •λ³΄λ¥Ό ν™•μΈν•  μ μμ**
    - `feature_importances_` : μ„¤λ…λ³€μλ³„ κ°€μ¤‘μΉ

- **λ‹¤μμ„ ν†µν•΄ ν•™μµλ λ¨λΈμ΄ κ³„μ‚°ν• μ„¤λ…λ³€μλ³„ κ°€μ¤‘μΉλ¥Ό μ‹κ°ν™”ν•  μ μμ**

    ```
    from lightgbm import plot_importance
    print(plot_importance(lgb_clf))
    ```

- **κΈ°νƒ€ ν•μ΄νΌνλΌλ―Έν„° λ° λ¨λΈ μ •λ³΄ λ©λ΅μ€ `estimator_params`μ„ ν†µν•΄ ν™•μΈ κ°€λ¥ν•¨**

</details>