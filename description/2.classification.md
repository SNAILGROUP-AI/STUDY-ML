## 🤓 분류분석이란 무엇인가

- **정의 : 데이터를 나누는 발산 작업**
    - 대상의 정체를 규명하는 작업
    - 데이터를 일정한 기준에 따라 몇 가지 범주로 나누는 작업
    - 반응변수가 범주형 변수인 경우

- **학습 양상**
    - 분류분석 시 학습은 데이터를 잘 분류할 수 있는 함수를 찾는 행위임
    - 함수의 형태는 수식이 될 수도 있고, 규칙이 될 수도 있음
    - 이상적인 분류기는 새로운 데이터를 잘 규명할 수 있는 분류기

- **이슈 : 어떠한 기준으로 분류할 것인가** 

- **구분**
    - **이항분류분석(binary classification)** : 대상을 둘 중 하나로 분류하는 분석 방법
    - **다항분류분석(multi-category classificaiton)** : 대상을 3개 이상의 범주 중 하나로 분류하는 분석 방법

---

## 🛠️ 주요 알고리즘

<details><summary><h3>결정 트리</h3></summary>

- **정의 : 데이터에 내재된 규칙을 발견하여 수형도 기반의 분류 규칙을 세우고 데이터를 분류하는 알고리즘**
    
- **주요 이슈 : 트리를 어떻게 분할할 것인가**
    - 가지를 몇 번 뻗을 것인가
    - 한 범주당 데이터가 몇 개 남았을 때 가지치기를 멈출 것인가
    
- **주의 사항**
    - node가 깊어질수록 성능이 저하될 수 있음
    - 범주마다 균일한 데이터 세트를 구성할 수 있도록 하이퍼파라미터를 설정해야 함

- **구조**

    다운로드.png

    - **root node** : 최상위 노드
    - **decision node** : 규칙 노드
    - **leaf node** : 최종 범주
    - **gini** : 데이터 분포의 균일도
    - **samles** : 임의의 규칙에 대하여 해당 규칙을 만족하는 데이터 건수
    - **value** : 각 범주의 데이터 건수

- **균일도**
    - 정의 : leaf node에 각 범주에 해당하는 데이터만 포함되어 있는가
    
    - 예시
        - `color`을 기준으로 바둑알을 구분한다고 가정하자
        - 범주로는 `black` , `white` 가 존재함
        - 범주 `black`에 검정색 바둑알만 포함되어 있다면 균일도가 높다고 해석함
        - 범주 `black`에 흰색 바둑알이 많이 섞여 있을수록 균일도가 낮다고 해석함
    
    - decision node에서는 균일도가 높은 데이터 셋을 먼저 분류할 수 있도록 규칙을 구성함

- **지니계수**
    - 정의 : 균일도를 측정하는 방법 혹은 불순도를 측정하는 방법
        - 지니계수가 높을수록 균일도가 낮고, 불순도가 높다고 해석함
        - 본래 경제학에서 불평등 지수를 나타낼 때 사용하는 지수였음
        - 0에 가까울수록 평등하고, 1에 가까울수록 불평등하다고 해석했음
    
    - 결정 트리는 지니계수를 낮추는 방향으로 가지치기를 진행함
        - 데이터 셋을 분할하는 데 가장 좋은 조건인 지니계수가 낮은 조건을 찾음
        - 해당 조건에 기초하여 데이터 셋을 하위 노드에 반복적으로 분할함
        - 모든 데이터가 특정 범주에 속하게 되면 분할을 중지함

- **사용 방법**

</details>

<details><summary><h3>최근접 이웃</h3></summary>

- **정의 : 기하학적 거리를 규칙으로 하여 데이터를 분류하는 알고리즘**
    - 임의의 설명변수 조합이 나타내는 좌표평면 상의 한 점에 대하여,
    - 해당 점과 가장 가깝게 위치하는 점이 의미하는 설명변수 조합의 범주로 분류함

- **주요 이슈 : 참조할 이웃의 수를 얼마로 설정할 것인가**

    ![최근접이웃](https://miro.medium.com/max/405/0*QyWp7J6eSz0tayc0.png)

- **사용 방법**

</details>

<details><summary><h3>로지스틱 회귀</h3></summary>

</details>

---

## 💯 성능 평가 지표

<details><summary><h3>성능 평가란 무엇인가</h3></summary>

- **정의 : 오차를 기준으로 모델의 성능을 평가하는 절차**
    - 오차 : 예측값과 실제값의 차이
    - 오차를 0으로 만드는 것은 현실적으로 불가능하므로 오차를 허용할 범위를 결정해야 함
    - 실제값이 주어져야 오차를 측정할 수 있으므로 지도학습에서만 이루어짐

- **목적 : 과적합 방지 및 최적 모델 채택**
    - **과적합(overfitting) 판단 기준**
        - 학습용 데이터를 통해 수행한 예측 오차와 평가용 데이터를 통해 수행한 예측 오차 간 차이가 적을수록 과적합되지 않았다고 판단함
        - 학습용 데이터로는 성능이 높게 평가되었으나 평가용 데이터로는 성능이 낮게 평가되었다면 학습용 데이터에 과대적합된 상태임
        - 반대로 학습용 데이터로는 성능이 낮게 평가되었으나 평가용 데이터로는 성능이 높게 평가되었다면 학습용 데이터에 과소적합된 상태임

    - **최적 모델 판단 기준**
        - 과적합 문제가 해결된 여러 모델들 중에서 성능이 가장 높은 모델을 채택하는 기준으로 사용 가능함
        - 즉, 최적 알고리즘 및 최적 하이퍼파라미터 판단 기준으로서 사용 가능함

- **주요 지표**
    - 모델링 목적 또는 반응변수의 유형에 따라 다른 평가지표를 사용함
    - 분류분석(범주형 변수) : 정확도, F1-score 등
    - 회귀분석(연속형 변수) : mse, rmse, mae, mape 등

</details>

<details><summary><h3>오차행렬</h3></summary>

- **오차행렬(confusion matrix)**

    ![오차행렬](https://miro.medium.com/max/1400/1*4c3YSE9UrrmulLu0K66g1Q.png)
    - 분류분석 결과 예측 범주와 실제 범주를 교차 표(cross table) 형태로 정리한 행렬
    - 이항분류분석에 대하여 예측 오류가 얼마인지와 더불어 어떠한 유형의 예측 오류가 발생할 수 있는지 나타냄

- **해석**
    - **True Positive(TP)** : 예측값이 1이고, 실제값도 1인 경우
    - **True Negative(TN)** : 예측값이 0이고, 실제값도 0인 경우
    - **False Positive(FP)** : 제1종 오류; 예측값이 1이고, 실제값은 0인 경우
    - **False Nagative(FN)** : 제2종 오류; 예측값이 0이고, 실제값은 1인 경우

- **정확도, 오류율, 정밀도, 재현율**
    ![](https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg)

    - **정확도** : (TP + TN) / (TP + TN + FP + FN)
    - **오류율** : (FP + FN) / (TP + TN + FP + FN)
    - **정밀도** : TP / (TP + FP)
    - **재현율** : TP / (TP + FN)

</details>

<details><summary><h3>정확도(Accuracy)</h3></summary>

- **정의 : 실제 데이터와 예측 데이터가 얼마나 동일한지를 평가 기준으로 하는 지표**
    - 전체 예측 개수 대비 정확하게 예측한 개수
    - 0~1 사이의 값을 가지며 1에 가까울수록 성능이 우수하다고 평가함

- **주의 : 반응변수의 범주 간 개수가 불균형한 데이터 셋의 경우 활용하기에 적합하지 않음**
    - 가령 이항분류분석에서 참인 것의 개수가 99이고 거짓인 것의 개수가 1이라고 하자
    - 무조건 참으로 예측하면 0.99의 정확도를 가지게 됨

</details>

<details><summary><h3>F1-Score</h3></summary>

- **정의 : 정밀도와 재현율의 조화 평균**
    - 정밀도와 재현율 중 어느 한쪽으로 치우치지 않을수록 높은 값을 가짐
    - 0~1 사이의 값을 가지며 1에 가까울수록 성능이 우수하다고 평가함

- **정밀도**
    - 참으로 예측한 것의 개수 대비 정확하게 예측한 개수
    - 0~1 사이의 값을 가지며 1에 가까울수록 성능이 우수하다고 평가함
    - 제1종 오류가 문제되는 경우 주요한 지표로서 사용됨
    - 즉, 실제 거짓인 데이터를 참으로 판단하면 큰 문제가 발생하는 경우

- **재현율**
    - 참인 것의 개수 대비 참으로 예측한 것의 개수
    - 0~1 사이의 값을 가지며 1에 가까울수록 성능이 우수하다고 평가함
    - 민감도(sensitivity)라고도 부름
    - 제2종 오류가 문제되는 경우 주요한 지표로서 사용됨
    - 즉, 실제 참일 데이터를 거짓으로 판단하면 큰 문제가 되는 경우
    - 모델 안정성을 평가하는 지표로서 사용됨

- **정밀도와 재현율의 관계**
    - **정밀도와 재현율은 모두 TP를 높이는 것을 목적으로 함**
        - 단, 정밀도는 제1종 오류에 초점을 맞추는 지표로서 FP를 낮추는 방향으로 TP를 높이고자 함
        - 반면, 재현율은 제2종 오류에 초점을 맞추는 지표로서 FN를 낮추는 방향으로 TP를 높이고자 함

    - **정밀도와 재현율은 Trade-off 관계라고 볼 수 있음**
        - FP와 FN 중 어느 한쪽의 수치를 강제로 높이면 다른 한쪽의 수치를 낮추기 쉬워짐
        - 가령 어떤 자료가 참일 확률이 0.9라면 참으로 에측할 가능성이 매우 높음
        - 반면, 어떤 자료가 참일 확률이 0.1이라면 거짓으로 예측할 가능성이 매우 높음

    - **정밀도와 재현율의 조화평균이 가지는 의미**
        - 그렇다면 어떤 자료가 참일 확률이 0.6이라면 참과 거짓 중 무엇으로 분류해야 하는가
        - 임계값(threshold)을 기준으로 분류할 수 있음
        - F1-Score은 정확도와 재현율 중 어느 한쪽을 희생하지 않고서 양쪽을 모두 높이는 임계치임

</details>

<details><summary><h3>AUC</h3></summary>

- **ROC 곡선**
    - **민감도(True Positive Rate; TPR)** :  참인 것에 대하여 참으로 예측한 비율
    - **특이도(True Negative Rate; TNR)** : 거짓인 것에 대하여 거짓으로 예측한 비율
    - **ROC 곡선** : (1 - TNR)의 변화에 따른 TPR의 변화 양상을 나타내는 곡선
    - (x, y) == (0, 1)일 때 성능이 가장 좋음

- **민감도와 특이도는 trade-off 관계임**
    - 모든 자료를 참으로 예측하는 경우 민감도를 최대치로 가져갈 수 있음
    - 반면, 이러한 경우 특이도를 최소치로 가져가게 됨

- **AUC(Area Under Curve)**
    - 정의 : ROC 곡선과 X축으로 둘러싸인 면적의 너비
    - 해석 : 0.5~1의 값을 가지며 1에 가까울수록 성능이 우수하다고 평가함
    - 척도 불변(Scale-Invariant) : 절대값이 아니라 비율을 통해서 성능을 평가함
    - 분류 임계값 불변(Classification-Threshold-Invariant) : 어떤 분류 임계값으로 무엇을 선택했는지와 무관하게 성능을 평가함

</details>

---

## 📝 Practice

- [**실습 코드**]()

- [**데이터 명세서**]()