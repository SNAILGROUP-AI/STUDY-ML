## âœ‚ï¸ ë°ì´í„° ì„¸íŠ¸ ë‚˜ëˆ„ê¸°

<details><summary><h3>ë°ì´í„° ì„¸íŠ¸ ë‚˜ëˆ„ê¸°</h3></summary>

![ë°ì´í„° ì„¸íŠ¸ ë‚˜ëˆ„ê¸°](https://miro.medium.com/max/1400/0*DKB-pJy7-G6gEkM-)

- **ëª©ì  : ì¸ìŠ¤í„´ìŠ¤ í›ˆë ¨ì— ì‚¬ìš©í•  ë°ì´í„° ì„¸íŠ¸ì™€ ì„±ëŠ¥ í‰ê°€ì— ì‚¬ìš©í•  ë°ì´í„° ì„¸íŠ¸ë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•¨**
    
- **ìš©ë„**
    - **í›ˆë ¨ìš©(train)** : ì¸ìŠ¤í„´ìŠ¤ í›ˆë ¨(í˜¹ì€ í•™ìŠµ) ìš©ë„
    - **ê²€ì¦ìš©(validation)** : ê³¼ì í•©ì„ ì¶©ë¶„íˆ ë°©ì§€í•˜ì—¬ í›ˆë ¨ì„ ì¤‘ë‹¨í•´ë„ ë¬´ë°©í•œì§€ íŒë‹¨ ìš©ë„
    - **í‰ê°€ìš©(test)** : ì¸ìŠ¤í„´ìŠ¤ ì„±ëŠ¥ í‰ê°€ ìš©ë„ë¡œì„œ í›ˆë ¨ ì‹œ ì‚¬ìš©ë˜ì§€ ì•Šì€ ë ˆì½”ë“œ

- **ì‚¬ìš© ë°©ë²•**

    ```
    from sklearn.model_selection import train_test_split

    # ë°ì´í„° ì„¸íŠ¸ë¥¼ ì„¤ëª…ë³€ìˆ˜ ì¡°í•© Xì™€ ë°˜ì‘ë³€ìˆ˜ yë¡œ êµ¬ë¶„í•¨
    X = df.drop(columns = [target])
    y = df[[target]]

    # ë°ì´í„° ì„¸íŠ¸ë¥¼ í›ˆë ¨ìš©ê³¼ í‰ê°€ìš©ìœ¼ë¡œ ë¶„ë¦¬í•¨
    # í›ˆë ¨ìš© ë°ì´í„° ì„¸íŠ¸ë¥¼ (X_train, y_train), í‰ê°€ìš© ë°ì´í„° ì„¸íŠ¸ë¥¼ (X_test, y_test)ì— í• ë‹¹í•¨
    X_train, X_test, y_train, y_test = train_test_split(X, y)
    ```

- **ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°**
    - `random_state = None`

    - `test_size = 0.25` : ì „ì²´ ë°ì´í„° ì„¸íŠ¸ ëŒ€ë¹„ í‰ê°€ìš© ë°ì´í„° ì„¸íŠ¸ì˜ ë¹„ì¤‘
    
    - `shuffle = True` : í›ˆë ¨ìš©ê³¼ í‰ê°€ìš©ìœ¼ë¡œ ë¶„ë¦¬í•˜ê¸° ì „ì— ë ˆì½”ë“œë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ì„ ê²ƒì¸ê°€
    
    - `stratify = None` : ë²”ì£¼ì˜ ë¹„ìœ¨ì„ í›ˆë ¨ìš©ê³¼ í‰ê°€ìš©ì—ë„ ìœ ì§€í•  ë²”ì£¼í˜• ë³€ìˆ˜ ëª©ë¡
        - ë¶„ë¥˜ë¶„ì„ ì‹œ ë°˜ì‘ë³€ìˆ˜ì— ëŒ€í•˜ì—¬ ì„¤ì •í•  ê²ƒì„ ê¶Œì¥í•¨

</details>

---

## ğŸ”¢ Numerical Feature Engineering

<details><summary><h3>ì´ìƒì¹˜ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°</h3></summary>

- **ì´ìƒì¹˜(Outlier)**
    - **ì •ì˜ : ê´€ì¸¡ëœ ë°ì´í„°ì˜ ë²”ìœ„ì—ì„œ ì§€ë‚˜ì¹˜ê²Œ ë²—ì–´ë‚˜ ê°’ì´ ë§¤ìš° í¬ê±°ë‚˜ ì‘ì€ ê°’**

    - **ì´ìƒì¹˜ì˜ íŒë³„**
        - ì œ1ì‚¬ë¶„ìœ„ìˆ˜ì™€ ì œ3ì‚¬ë¶„ìœ„ìˆ˜ê°€ ìƒì‹ê³¼ ë¶€í•©í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ë°ì´í„° ì„¸íŠ¸ê°€ ì˜ëª»ëœ ê²ƒìœ¼ë¡œ íŒë‹¨í•¨
        - boxplot ë“± ë¶„í¬ ì‹œê°í™” íˆ´ì„ í™œìš©í•˜ì—¬ ì´ìƒì¹˜ ì¡´ì¬ ê°€ëŠ¥ì„± ì—¬ë¶€ë¥¼ í™•ì¸í•¨
        - ì´ìƒì¹˜ê°€ ì¡´ì¬í•  ê°€ëŠ¥ì„±ì´ ìˆë‹¤ê³  íŒë‹¨ë˜ë©´ ì´ìƒì¹˜ íƒì§€ ê¸°ë²•ì„ í†µí•´ ì´ìƒì¹˜ë¥¼ ê·œì •í•˜ê³  ì²˜ë¦¬í•¨

    - **ì´ìƒì¹˜ì˜ íƒì§€ : Turkey Fence ê¸°ë²•**
        - ì •ì˜ : ì‚¬ë¶„ìœ„ ë²”ìœ„(InterQuartile Range; IQR)ì„ í™œìš©í•˜ì—¬ ì´ìƒì¹˜ë¥¼ íŒë³„í•˜ëŠ” ê¸°ë²•
            - **ì‚¬ë¶„ìœ„ ë²”ìœ„(IQR)** : ì œ3ì‚¬ë¶„ìœ„ìˆ˜(Q3) - ì œ1ì‚¬ë¶„ìœ„ìˆ˜(Q1)

        - ì´ìƒì¹˜ë¥¼ ìƒí•œê°’ì„ ì´ˆê³¼í•˜ê±°ë‚˜ í•˜í•œê°’ì— ë¯¸ë‹¬í•œ ê°’ìœ¼ë¡œ ê·œì •í•¨
            - **í•˜í•œê°’(lower_value)** : $Q1-IQR \times 1.5$
            - **ìƒí•œê°’(upper_value)** : $Q3+IQR \times 1.5$

    - **ì´ìƒì¹˜ì˜ ì²˜ë¦¬ : í†µìƒì ìœ¼ë¡œëŠ” ìƒí•œê°’ ë° í•˜í•œê°’ìœ¼ë¡œ ëŒ€ì²´í•¨**

- **ì‚¬ìš© ë°©ë²•**

    ```
    from sklearn.preprocessing import RobustScaler
    
    col = "ì´ìƒì¹˜ë¥¼ ì²˜ë¦¬í•  ì»¬ëŸ¼ëª…"
    before_scaled = X[[col]]

    # Turkey Fence ê¸°ë²•ì— ê¸°ë°˜í•œ ì´ìƒì¹˜ íƒì§€ ë° ì²˜ë¦¬ê¸° RobustScaler ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    scaler = RobustScaler()

    # ì´ìƒì¹˜ íƒì§€
    scaler.fit(before_scaled)

    # ì´ìƒì¹˜ ì²˜ë¦¬
    after_scaled = scaler.transform(before_scaled)

    # ì´ìƒì¹˜ ì²˜ë¦¬ ì „í›„ ë¹„êµ
    before_scaled = before_scaled.rename(columns = {col : "before"})
    after_scaled = after_scaled.rename(columns = {col : "after"})
    scale_df = pd.concat([before_scaled, after_scaled], axis = 1)

    print(scale_df)
    ```

- **ë‹¤ìŒì„ í†µí•´ ìŠ¤ì¼€ì¼ëŸ¬ì˜ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŒ**
    - `center_` : ì¤‘ì•™ê°’
    - `scale_` : ì‚¬ë¶„ìœ„ ë²”ìœ„

</details>

<details><summary><h3>ë¶„í¬ê°€ ë“¤ì‘¥ë‚ ì‘¥í•œ ê²½ìš°</h3></summary>

- **í‘œì¤€í™”(Standardization)**

    ![stanard](https://user-images.githubusercontent.com/116495744/222760130-bdcce494-0d8b-407c-8859-6ab6524b6127.jpg)

    ### $$x_{new}=\frac{x_i-mean(x)}{std(x)}$$

    - ì •ì˜ : ê°’ì˜ ë¶„í¬ë¥¼ í‰ê· ì´ 0, ë¶„ì‚°ì´ 1ì¸ í‘œì¤€ì •ê·œë¶„í¬(ê°€ìš°ì‹œì•ˆ ì •ê·œ ë¶„í¬) í˜•íƒœë¡œ ë³€í™˜í•¨
    - ëª©ì  : ëª¨ë“  ì„¤ëª…ë³€ìˆ˜ì˜ í˜•íƒœë¥¼ í†µê³„ ë¶„ì„ì˜ ê°€ì •ì— ë¶€í•©í•˜ëŠ” í˜•íƒœë¡œ ë³€í™˜í•¨

- **ì‚¬ìš© ë°©ë²•**

    ```
    from sklearn.preprocessing import StandardScaler
    
    col = "í‘œì¤€í™”í•  ì»¬ëŸ¼ëª…"
    before_scaled = X[[col]]

    # í‘œì¤€í™” ì²˜ë¦¬ê¸° StandardScaler ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    scaler = StandardScaler()

    # í‰ê·  ë° ë¶„ì‚° íƒìƒ‰
    scaler.fit(before_scaled)

    # í‘œì¤€í™”
    after_scaled = scaler.transform(before_scaled)

    # í‘œì¤€í™” ì „í›„ ë¹„êµ
    before_scaled = before_scaled.rename(columns = {col : "before"})
    after_scaled = after_scaled.rename(columns = {col : "after"})
    scale_df = pd.concat([before_scaled, after_scaled], axis = 1)

    print(scale_df)
    ```

</details>

<details><summary><h3>ë‹¨ìœ„ê°€ ë“¤ì‘¥ë‚ ì‘¥í•œ ê²½ìš°</h3></summary>

- **ì •ê·œí™”(Normalization)**

    ![minmax](https://user-images.githubusercontent.com/116495744/222760155-d4fc55ff-3959-4b12-9acb-577c632ad958.jpg)

    ### $$x_{new}=\frac{x_i-min(x)}{max(x)-min(x)}$$

    - ì •ì˜ : ê°’ì˜ ë²”ìœ„ë¥¼ íŠ¹ì •í•˜ê³  ëª¨ë“  ì„¤ëª…ë³€ìˆ˜ì˜ ë¶„í¬ë¥¼ í•´ë‹¹ ë²”ìœ„ë¡œ í™•ëŒ€ í˜¹ì€ ì¶•ì†Œí•¨
    - ëª©ì  : ëª¨ë“  ì„¤ëª…ë³€ìˆ˜ì˜ í¬ê¸°ë¥¼ í†µì¼í•˜ì—¬ ì„¤ëª…ë³€ìˆ˜ ê°„ ìƒëŒ€ì  í¬ê¸°ê°€ ì£¼ëŠ” ì˜í–¥ë ¥ì„ ìµœì†Œí™”í•¨

- **ì‚¬ìš© ë°©ë²•**

    ```
    from sklearn.preprocessing import MinMaxScaler
    
    col = "ì •ê·œí™”í•  ì»¬ëŸ¼ëª…"
    before_scaled = X[[col]]

    # ì •ê·œí™” ì²˜ë¦¬ê¸° MinMaxScaler ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    scaler = MinMaxScaler()

    # ìµœëŒ€ìµœì†Œ ë³€í™˜ì„ ìœ„í•œ ë¶„í¬ íƒìƒ‰
    scaler.fit(before_scaled)

    # ì •ê·œí™”
    after_scaled = scaler.transform(before_scaled)

    # ì •ê·œí™” ì „í›„ ë¹„êµ
    before_scaled = before_scaled.rename(columns = {col : "before"})
    after_scaled = after_scaled.rename(columns = {col : "after"})
    scale_df = pd.concat([before_scaled, after_scaled], axis = 1)

    print(scale_df)
    ```

</details>

<details><summary><h3>ìˆ˜ì¹˜í˜• ì„¤ëª…ë³€ìˆ˜ì˜ ì „ì²˜ë¦¬ ìˆœì„œ</h3></summary>

![ìŠ¤ì¼€ì¼ë§ ë¹„êµ](https://miro.medium.com/max/1400/1*0Ox-p57oxfmaVSaJyJWyPg.png)

- **`RobustScaler` ğŸ‘‰  `StandardScaler` ğŸ‘‰ `MinMaxScaler` ìˆœì„ ê¶Œì¥í•¨**

    - ì´ìƒì¹˜ê°€ ì¡´ì¬í•  ê²½ìš° ì •ê·œí™”ì— ë”°ë¥¸ ì„±ëŠ¥ ê°œì„  íš¨ê³¼ê°€ ë¯¸ë¯¸í•¨
    - ì •ê·œí™” ì´í›„ í‘œì¤€í™”ë¥¼ í•˜ëŠ” ê²½ìš° ì„¤ëª…ë³€ìˆ˜ë³„ ë²”ìœ„ê°€ ì¬ì¡°ì •ë  ê°€ëŠ¥ì„±ì´ ìˆìŒ

</details>

---

## ğŸ”¤ Categorical Feature Engineering

<details><summary><h3>ë ˆì´ë¸” ì¸ì½”ë”©</h3></summary>

- **ì‚¬ìš© ë°©ë²•**

    ```
    from sklearn.preprocessing import LabelEncoder

    col = "ì¸ì½”ë”©í•  ì»¬ëŸ¼ëª…"
    before_encoded = X[[col]]

    # ë ˆì´ë¸” ì¸ì½”ë” LabelEncoder ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    label = LabelEncoder()

    # ë²”ì£¼ íƒìƒ‰
    label.fit(before_encoded)

    # ë ˆì´ë¸” ì¸ì½”ë”©
    after_label = label.transform(before_encoded)
    
    # ë ˆì´ë¸” ì¸ì½”ë”© ì „í›„ ë¹„êµ
    before_encoded = before_encoded.rename(columns = {col : "before"})
    after_label = after_label.rename(columns = {col : "label"})
    encode_df = pd.concat([before_encoded, after_label], axis = 1)

    print(encode_df)
    ```

- **ë‹¤ìŒì„ í†µí•´ ì¸ì½”ë”ì˜ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŒ**
    - `classes_` : ìˆ«ìë³„ ë§¤ì¹­ë˜ì–´ ìˆëŠ” ë²”ì£¼ëª…
    - `inverse_transform(xs)` : ë¦¬ìŠ¤íŠ¸ $xs$ì— ëŒ€í•˜ì—¬ ê·¸ ì›ì†Œë“¤ì„ ìˆœì°¨ë¡œ ì—­ì¸ì½”ë”©í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•œ í›„ í•´ë‹¹ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•¨

</details>

<details><summary><h3>ì› í•« ì¸ì½”ë”©</h3></summary>

- **ì‚¬ìš© ë°©ë²•**

    ```
    from sklearn.preprocessing import OneHotEncoder

    # ì› í•« ì¸ì½”ë” OneHotEncoder ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    oht = OneHotEncoder()

    # ë ˆì´ë¸” ì¸ì½”ë”©í•œ 3ì°¨ì› í–‰ë ¬ after_labelì„ 2ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
    before_oht = after_label.reshape(-1, 1)

    # ë²”ì£¼ íƒìƒ‰
    oht.fit(before_oht)

    # ì› í•« ì¸ì½”ë”©
    after_oht = oht.transform(before_oht)

    # ê²°ê³¼ë¥¼ í¬ì†Œí–‰ë ¬ í˜•íƒœì—ì„œ ë°€ì§‘í–‰ë ¬ í˜•íƒœë¡œ ë³€í™˜
    after_oht = after_oht.toarray()


 
    before_scaled = before_scaled.rename(columns = {col : "before"})
    after_scaled = after_scaled.rename(columns = {col : "after"})
    encode_df = pd.concat([before_scaled, after_scaled], axis = 1)

    print(scale_df)

    print(scale_df)
    for col in cat_col :
        xs = df[col]
        
        label = LabelEncoder()
        xs = label.fit_transform(xs)
        
        xs = xs.reshape(-1, 1)
        
        oht = OneHotEncoder()
        xs = oht.fit_transform(xs)
        
        xs = xs.toarray()

        label_list = list(label.classes_)
        label_list = [col + "_" + label_list[i] for i in range(len(label_list))]
        
        encoded_col = pd.DataFrame(xs, columns = label_list)
        encoded_list.append(encoded_col)

    encoded_df = pd.concat(encoded_list, axis = 1)
    df = pd.concat([df, encoded_df], axis = 1)
    df = df.drop(columns = cat_col)
    ```


</details>

---

## â˜‘ï¸ Feature Selecting

<details><summary><h3>ë¶„ë¥˜ë¶„ì„ - ìŠ¹ì‚°ë¹„ ê¸°ì¤€</h3></summary>

- **ìŠ¹ì‚°ë¹„ì˜ ì´í•´**
    - **ìŠ¹ì‚°(odds)**
        - ì´í•­ë²”ì£¼í˜• ë°˜ì‘ë³€ìˆ˜ì— ëŒ€í•˜ì—¬ ë°˜ì‘í•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±($1-p$) ëŒ€ë¹„ ë°˜ì‘í•  ê°€ëŠ¥ì„±($p$)
        - ë°˜ì‘ë³€ìˆ˜ê°€ ë°˜ì‘í•  ê°€ëŠ¥ì„±ì´ ë°˜ì‘í•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ë³´ë‹¤ ëª‡ ë°° ë†’ì€ê°€
        - ë°˜ì‘ë³€ìˆ˜ê°€ ë°˜ì‘í•  ê°€ëŠ¥ì„±ì„ $p$ ë¼ê³  í–ˆì„ ë•Œ, ìŠ¹ì‚° $odds$ ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ
        
        ### $$odds=\frac{p}{1-p}$$

    - **ìŠ¹ì‚°ë¹„(Oods Ratio; OR)**
        - ì´í•­ë²”ì£¼í˜• ë°˜ì‘ë³€ìˆ˜ yì™€ ì´í•­ë²”ì£¼í˜• ì„¤ëª…ë³€ìˆ˜ xì— ëŒ€í•˜ì—¬ xì˜ ë³€ë™ì— ë”°ë¥¸ yì˜ ë°˜ì‘
        
        - ì„¤ëª…ë³€ìˆ˜ê°€ ì°¸ì¼ ë•Œ ë°˜ì‘ë³€ìˆ˜ê°€ ë°˜ì‘í•  ê°€ëŠ¥ì„±ì´ ê±°ì§“ì¼ ë•Œë³´ë‹¤ ëª‡ ë°° ë†’ì€ê°€
        
        - ì´í•­ë²”ì£¼í˜• ë°˜ì‘ë³€ìˆ˜ yì™€ ì´í•­ë²”ì£¼í˜• ì„¤ëª…ë³€ìˆ˜ xì— ëŒ€í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ ê°€ì •í•˜ì
            - xê°€ ì°¸ì¼ ë•Œ yê°€ ë°˜ì‘í•  í™•ë¥  : $a$
            - xê°€ ì°¸ì¼ ë•Œ yê°€ ë°˜ì‘í•˜ì§€ ì•Šì„ í™•ë¥  : $b$
            - xê°€ ê±°ì§“ì¼ ë•Œ yê°€ ë°˜ì‘í•  í™•ë¥  : $c$
            - xê°€ ê±°ì§“ì¼ ë•Œ yê°€ ë°˜ì‘í•˜ì§€ ì•Šì„ í™•ë¥  : $d$
            - $a+b+c+d=1$
        
        - xì— ëŒ€í•œ yì˜ ìŠ¹ì‚°ë¹„ $OR$ ì€ ë‹¤ìŒê³¼ ê°™ìŒ
        
        ### $$OR=\frac{a/b}{c/d}$$
    
    - **ìŠ¹ì‚°ë¹„ì˜ í•´ì„**
        - $OR \approx 1$ : í•´ë‹¹ ì„¤ëª…ë³€ìˆ˜ì™€ ë°˜ì‘ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ê°€ ìœ ì˜ë¯¸í•˜ì§€ ì•Šë‹¤ê³  íŒë‹¨í•¨
        - $OR < 1$ : í•´ë‹¹ ì„¤ëª…ë³€ìˆ˜ì™€ ë°˜ì‘ë³€ìˆ˜ ê°„ ìŒì˜ ìƒê´€ê´€ê³„ê°€ ìˆë‹¤ê³  íŒë‹¨í•¨
        - $OR > 1$ : í•´ë‹¹ ì„¤ëª…ë³€ìˆ˜ì™€ ë°˜ì‘ë³€ìˆ˜ ê°„ ì–‘ì˜ ìƒê´€ê´€ê³„ê°€ ìˆë‹¤ê³  íŒë‹¨í•¨

- **ë¡œì§€ìŠ¤í‹± íšŒê·€ì‹ì˜ ê°€ì¤‘ì¹˜ì˜ ì´í•´**
    - ë‹¨ìˆœíšŒê·€ë¶„ì„ í•˜ì˜ ë¡œì§€ìŠ¤í‹± íšŒê·€ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŒ

    ### $$ln(\frac{p}{1-p})=w_0+wX$$
    
    - ì´í•­ë²”ì£¼í˜• ë°˜ì‘ë³€ìˆ˜ yì™€ ì´í•­ë²”ì£¼í˜• ì„¤ëª…ë³€ìˆ˜ Xì— ëŒ€í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ ê°€ì •í•˜ì
        - xê°€ ì°¸ì¼ ë•Œ yê°€ ë°˜ì‘í•  í™•ë¥  : $a$
        - xê°€ ì°¸ì¼ ë•Œ yê°€ ë°˜ì‘í•˜ì§€ ì•Šì„ í™•ë¥  : $b$
        - xê°€ ê±°ì§“ì¼ ë•Œ yê°€ ë°˜ì‘í•  í™•ë¥  : $c$
        - xê°€ ê±°ì§“ì¼ ë•Œ yê°€ ë°˜ì‘í•˜ì§€ ì•Šì„ í™•ë¥  : $d$
        - $a+b+c+d=1$

    - Xê°€ ì°¸(1)ì¼ ë•Œì˜ íšŒê·€ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŒ
    
    ### $$ln(\frac{a}{b})=w_0+w$$

    - Xê°€ ê±°ì§“(0)ì¼ ë•Œì˜ íšŒê·€ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŒ

    ### $$ln(\frac{c}{d})=w_0$$

    - ë‘ íšŒê·€ì‹ì„ ë¹¼ë©´ ë‹¤ìŒê³¼ ê°™ìŒ

    ### $$ln(\frac{a/b}{c/d})=w$$

    - ë”°ë¼ì„œ Xì— ëŒ€í•œ yì˜ ìŠ¹ì‚°ë¹„ì™€ Xì˜ ê°€ì¤‘ì¹˜ w ê°„ì—ëŠ” ë‹¤ìŒì˜ ê´€ê³„ê°€ ì„±ë¦½í•¨

    ### $$ln(\frac{a/b}{c/d})=w$$

- **ê²°ë¡ **
    - ìŠ¹ì‚°ë¹„ì˜ ì‹ ë¢°êµ¬ê°„ì— 1ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš° í•´ë‹¹ ì„¤ëª…ë³€ìˆ˜ì˜ ë³€ë™ì´ ë°˜ì‘ë³€ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ë ¥ì´ ìœ ì˜ë¯¸í•˜ì§€ ì•Šë‹¤ê³  íŒë‹¨í•¨
    - ì¦‰, ë¡œì§€ìŠ¤í‹± íšŒê·€ì‹ì—ì„œ ì„ì˜ì˜ ì„¤ëª…ë³€ìˆ˜ xì˜ ê°€ì¤‘ì¹˜ $w$ ë¥¼ ì§€ìˆ˜ë¡œ ê°€ì§€ëŠ” ì§€ìˆ˜í•¨ìˆ˜ $f(w)=e^w$ ì˜ ê°’ì— ëŒ€í•˜ì—¬
    - ê·¸ ì‹ ë¢°êµ¬ê°„ì— 1ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš° í•´ë‹¹ ì„¤ëª…ë³€ìˆ˜ê°€ ë°˜ì‘ë³€ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ë ¥ì´ ìœ ì˜ë¯¸í•˜ì§€ ì•Šë‹¤ê³  íŒë‹¨í•¨

- **ì‚¬ìš© ë°©ë²•**

    ```
    from sklearn.linear_model import LogisticRegression
    import scipy.stats as st

    # ë¡œì§€ìŠ¤í‹± íšŒê·€ ì•Œê³ ë¦¬ì¦˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    lg_clf = LogisticRegression()

    # ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ ìˆ˜í–‰
    lg_clf.fit(X, y)

    # ì„¤ëª…ë³€ìˆ˜, ê°€ì¤‘ì¹˜, ìŠ¹ì‚°ë¹„ ì •ë³´ë¥¼ ë‹´ì€ ë°ì´í„°í”„ë ˆì„ or_df ìƒì„±
    features = list(lg_clf.feature_names_in_)
    weights = list(lg_clf.coef_)
    odds_ratio = [np.exp(weights[i]) for i in range(weights)]

    or_dict = {
        'feature' : features,
        'weight' : weights,
        'or' : odds_ratio
    }

    or_df = pd.DataFrame(or_dict, index = 'feature')

    # ì‹ ë¢°ìˆ˜ì¤€ ì„¤ì •
    i = 0.95

    # 95% ì‹ ë¢°ìˆ˜ì¤€ í•˜ì—ì„œ ì„¤ëª…ë³€ìˆ˜ë³„ ìŠ¹ì‚°ë¹„ì˜ ì‹ ë¢°êµ¬ê°„ í™•ì¸
    # or_dfì— ìŠ¹ì‚°ë¹„ì˜ ìµœì†Œì¹˜ì™€ ìµœëŒ€ì¹˜ ì •ë³´ë¥¼ ë‹´ì€ ì¹¼ëŸ¼ ì¶”ê°€
    or_min_list = []
    or_max_list = []

    for feature in features :
        ci = st.norm.interval(
                alpha = i, 
                loc = or_df.loc[feature, 'or'], 
                scale = st.sem(X[feature])
                )
        
        or_min_list.append(ci[0])
        or_max_list.append(ci[1])
    
    or_df['or_min'] = or_min_list
    or_df['or_max'] = or_max_list

    # ì‹ ë¢°êµ¬ê°„ì— 1ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    # or_dfì— ì‹ ë¢°êµ¬ê°„ì— 1 í¬í•¨ ì—¬ë¶€ ì •ë³´ë¥¼ ë‹´ì€ ì¹¼ëŸ¼ ì¶”ê°€
    drop_list = []

    for feature in features :
        if (or_df.loc[feature, 'or_min'] <= 1) and (or_df.loc[feature, 'or_max'] >= 1) : drop_list.append(True)
        else : drop_list.append(False)
    
    or_df['drop'] = drop_list

    print(or_df)
    ```

</details>

<details><summary><h3>ì„ í˜•íšŒê·€ë¶„ì„ - ë‹¤ì¤‘ê³µì„ ì„± ê¸°ì¤€</h3></summary>

- **ë‹¤ì¤‘ê³µì„ ì„±(Multicollinearity)**
    - **ì •ì˜**
        - ì„ì˜ì˜ ë…ë¦½ë³€ìˆ˜ê°€ ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•˜ì—¬ ì œê³µí•˜ëŠ” ì •ë³´ê°€ ë‹¤ë¥¸ ë…ë¦½ë³€ìˆ˜ë“¤ì´ ì œê³µí•˜ëŠ” ì •ë³´ì— ëŒ€í•˜ì—¬ ê°€ì§€ëŠ” ì˜ì¡´ì„±
        - ì„ì˜ì˜ ë…ë¦½ë³€ìˆ˜ê°€ ë‹¤ì¤‘ê³µì„ ì„±ì´ ë†’ë‹¤ë©´, í•´ë‹¹ ë…ë¦½ë³€ìˆ˜ê°€ ì œê³µí•˜ëŠ” ì •ë³´ë¥¼ ë‹¤ë¥¸ ë…ë¦½ë³€ìˆ˜ë“¤ì´ ì œê³µí•˜ëŠ” ì •ë³´ë§Œìœ¼ë¡œ ìœ ì¶”í•  ìˆ˜ ìˆë‹¤ê³  íŒë‹¨í•¨

    - **ë‹¤ì¤‘ê³µì„ ì„±ì˜ íŒë‹¨**
        - **í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ë¥¼ í†µí•œ íŒë‹¨**
            - í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ë¥¼ í†µí•´ ì„¤ëª…ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ë¥¼ ì¸¡ì •í•¨
            - ë‘˜ ì‚¬ì´ì— ìƒê´€ê´€ê³„ê°€ ìœ ì˜ë¯¸í•˜ê²Œ ì¸¡ì •ë˜ë©´ ë‹¤ì¤‘ê³µì„ ì„±ì´ ìˆë‹¤ê³  íŒë‹¨í•¨
            - ë‹¨, ìƒê´€ê´€ê³„ê°€ ìœ ì˜ë¯¸í•˜ë‹¤ê³  íŒë‹¨í•˜ëŠ” ì¼ì •í•œ ê¸°ì¤€ì´ ì—†ìŒ

        - **ë¶„ì‚°íŒ½ì°½ê³„ìˆ˜(Variance Inflation Factor; VIF)ë¥¼ í†µí•œ íŒë‹¨**
            - ë‹¤ì¤‘ê³µì„ ì„±ì„ ì¸¡ì •í•œ ìˆ˜ì¹˜ë¡œì„œ ê·¸ ê°’ì´ ë†’ì„ìˆ˜ë¡ ë‹¤ì¤‘ê³µì„ ì„±ì´ ë†’ë‹¤ê³  íŒë‹¨í•¨
            - í†µìƒì ìœ¼ë¡œëŠ” 10ì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ë‹¤ì¤‘ê³µì„ ì„±ì´ ë†’ì€ í¸ì´ë¼ê³  ì—¬ê¹€
        
        - **í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ì™€ ë¶„ì‚°íŒ½ì°½ê³„ìˆ˜ ë¹„êµ**
            - í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ëŠ” ë‘ ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ ì¸¡ì •ì— ì´ˆì ì„ ë§ì¶¤
            - ë¶„ì‚°íŒ½ì°½ê³„ìˆ˜ëŠ” í•œ ë³€ìˆ˜ì˜ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì— ëŒ€í•œ ì˜ì¡´ì„± ì¸¡ì •ì— ì´ˆì ì„ ë§ì¶¤
            - ë”°ë¼ì„œ ë¶„ì‚°íŒ½ì°½ê³„ìˆ˜ê°€ ë‹¤ì¤‘ê³µì„ ì„±ì„ íŒë‹¨í•˜ê¸°ì— ë³´ë‹¤ ì í•©í•œ ì§€í‘œì„
        
    - **ë‹¤ì¤‘ê³µì„ ì„±ì˜ ì²˜ë¦¬**
        - **í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ë¥¼ í†µí•œ ì„¤ëª…ë³€ìˆ˜ ê°„ ì˜ì¡´ì„± í™•ì¸**
            - ì¼ì°¨ì ìœ¼ë¡œ í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ë¥¼ í†µí•´ ë‹¤ì¤‘ê³µì„ ì„±ì´ ì˜ì‹¬ë˜ëŠ” ë³€ìˆ˜ ë° í•´ë‹¹ ë³€ìˆ˜ê°€ ì˜ì¡´í•˜ê³  ìˆì„ ê²ƒìœ¼ë¡œ ì˜ì‹¬ë˜ëŠ” ë³€ìˆ˜ë¥¼ í™•ì¸í•¨
        
        - **ë¶„ì‚°íŒ½ì°½ê³„ìˆ˜ë¥¼ í†µí•œ ì„¤ëª…ë³€ìˆ˜ ì„ ë³„**
            - ì´ì°¨ì ìœ¼ë¡œ ë¶„ì‚°íŒ½ì°½ê³„ìˆ˜ë¥¼ í†µí•´ ë‹¤ì¤‘ê³µì„ ì„±ì´ ê°€ì¥ ë†’ë‹¤ê³  íŒë‹¨ëœ ë³€ìˆ˜ë¥¼ ì‚­ì œí•¨
            - ëª¨ë“  ì„¤ëª…ë³€ìˆ˜ì˜ ë¶„ì‚°íŒ½ì°½ê³„ìˆ˜ê°€ 10 ë¯¸ë§Œì´ ë  ë•Œê¹Œì§€ ë°˜ë³µí•¨

- **ì„¤ëª…ë³€ìˆ˜ ê°„ í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ì‹œê°í™”**

    - **íˆíŠ¸ë§µ**

        ```
        import matplotlib.pyplot as plt
        import seaborn as sns
        %matplotlib inline

        # ì„¤ëª…ë³€ìˆ˜ ì„¸íŠ¸ Xì˜ ê° ì»¬ëŸ¼ì— ëŒ€í•˜ì—¬ í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        X_corr = X.astype(float).corr()

        # íˆíŠ¸ë§µ í¬ê¸° ì„¤ì •
        plt.figure(figsize = (25, 12))

        # íŒ”ë ˆíŠ¸ ì„¤ì •
        colormap = plt.cm.Reds

        # íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°
        sns.heatmap(
            X_corr,
            cmap = colormap,
            linewidths = 0.01, 
            linecolor = 'white', 
            vmax = 1.0, 
            vmin = -1.0,
            square = True,
            annot = True, 
            annot_kws = {"size" : 12}
            )

        plt.show()
        ```

    - **ì‚°ì ë„**

        ```
        import matplotlib.pyplot as plt
        import seaborn as sns
        %matplotlib inline

        # ì‚°ì ë„ í¬ê¸° ì„¤ì •
        plt.figure(figsize = (30, 30))

        # ì‚°ì ë„ ê·¸ë¦¬ê¸°
        sns.pairplot(X)

        plt.show()
        ```

- **ë¶„ì‚°íŒ½ì°½ê³„ìˆ˜ë¥¼ í†µí•œ ë³€ìˆ˜ ì„ ë³„**

    ```
    from statsmodels.stats.outliers_influence import variance_inflation_factor

    # ë¶„ì‚°íŒ½ì°½ê³„ìˆ˜ ì„ê³„ê°’ ì„¤ì •
    i = 10
    
    # ëª¨ë“  ì„¤ëª…ë³€ìˆ˜ì˜ ë¶„ì‚°íŒ½ì°½ê³„ìˆ˜ê°€ i ë¯¸ë§Œì´ ë  ë•Œê¹Œì§€ ë¶„ì‚°íŒ½ì°½ê³„ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ì„¤ëª…ë³€ìˆ˜ë¥¼ ì œê±°í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•¨
    while True :
        vif = pd.DataFrame()
        vif['feature'] = X.columns
        vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
        vif_max = vif['VIF'].max()
        vif_max_col = vif[vif['VIF'] == vif_max].loc[:, 'feature']
        
        if vif_max >= i : X = X.drop(vif_max_col, axis = 1)
        else : break

    # ìµœì¢… ì„¤ëª…ë³€ìˆ˜ë“¤ì˜ ë¶„ì‚°íŒ½ì°½ê³„ìˆ˜ í™•ì¸
    print(vif)

    # ì„¤ëª…ë³€ìˆ˜ í™•ì¸
    print(X)
    ```

</details>

---

## ğŸ“Š Target Engineering

<details><summary><h3>ë¶„ë¥˜ë¶„ì„ ì‹œ ë°˜ì‘ë³€ìˆ˜ ë ˆì´ë¸” ê°„ ë ˆì½”ë“œ ë¶ˆê· í˜• ë¬¸ì œ</h3></summary>

- **ì‚¬ìš© ë°©ë²•**

    ```
    from imblearn.over_sampling import SMOTE

    # smote ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    sm = SMOTE(random_state = 121)

    # ë ˆì½”ë“œê°€ ë¶€ì¡±í•œ ë²”ì£¼ ë³µì œ
    X_train_over, y_train_over = sm.fit_resample(X_train, y_train)

    print(f'SMOTE ì ìš© ì „ í•™ìŠµìš© í”¼ì²˜/ë ˆì´ë¸” ë°ì´í„° ì„¸íŠ¸ : {X_train.shape}, {y_train.shape}')
    print(f'SMOTE ì ìš© í›„ í•™ìŠµìš© í”¼ì²˜/ë ˆì´ë¸” ë°ì´í„° ì„¸íŠ¸ : {X_train_over.shape}, {y_train_over.shape}')
    ```

</details>